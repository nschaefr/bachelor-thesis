\chapter{Konzeption}
Der Inhalt von Kapitel 3 stellt die Konzeption eines Testerstellungssystems namens \textit{Unitcraft} zur Bearbeitung des Hauptthemas dar. Dies geschieht in einer nachvollziehbaren Weise, sodass das erstellte Konzept reproduzierbar ist.

\section{Voraussetzungen}
Im Mittelpunkt der Arbeit befindet sich die Programmiersprache Java. Zur Vereinfachung der Analyse und Ausführung der generierten Tests werden die Java-Projekte auf das Build-Automation-Tool \textit{Apache Maven} beschränkt. Hierbei wird die klassische Maven Projektstruktur eingehalten. Diese besteht zunächst aus dem \textit{src}-Verzeichnis, welches sich in das \textit{main}-Verzeichnis, in dem sich der Java-Programmcode bzw. die dazugehörigen Ressourcen befinden, und in das \textit{test}-Verzeichnis unterteilt. \cite{MavenIntroductionStandard} [Abb. \ref{fig:dir}]\\ Somit wird eine klare Trennung zwischen Programmcode und Testcode gewährleistet und deutlich, in welchem Verzeichnis die Tests abgelegt werden müssen. \input{Assets/LaTeX/Dirtree.tex} \newpage
Um ein umfangreiches Generieren von Tests zu ermöglichen, ist die Einbindung von Frameworks als \textit{Dependencies} (engl. Abhängigkeiten) in der pom.xml essenziell. Neben dem Nutzen von JUnit-Jupiter wird ein Einbinden von Mockito vorausgesetzt, sodass die Verwendung von \textit{mocks} (engl. Attrappen) ermöglicht wird.\\
Zur Generierung von Metriken wird SonarQube in das Konzept eingebaut. Dabei wird eine weiteres \textit{Plugin} benötigt, um einen Coverage-Report erzeugen zu können. Die \textit{Java Code Coverage Library} (kurz JaCoCo) ermöglicht das Erstellen eines Code-Reports und stellt SonarQube alle notwendigen Daten bereit.
Die Implementierung von Plugins und Dependencies erfolgt im Kapitel \ref{chapter:impl}.

\section{Anforderungsanalyse}\label{section:anford}
Um das Testerstellungssystem praktikabel anwendbar zu gestalten, muss zunächst die Zielgruppe der Arbeit betrachtet werden. Es handelt sich dabei um Personen, die fachlich zugeordnet sind, und somit eine Wissensbasis im Bereich der Informatik besitzen. Aufgrunddessen wird auf eine nutzerfreundliche Oberfläche verzichtet und somit der Fokus auf die Programmlogik gelenkt.\\ Ein Testerstellungssystem als Kommandozeilen-Tool bietet hier eine vereinfachte Anwendung innerhalb des Projekts und eröffnet die Möglichkeit einer zukünftigen Einbindung in Automationsprozesse wie bspw. der \textit{Continuos Integration} (engl. kurz CI).\\ Die Wahl der Programmiersprache fällt hierbei auf Python. Python ist eine \textit{high-level}, interpretierte und dynamische Programmiersprache, welche Vorteile wie bspw. zahlreiche \textit{Libraries} (engl. Bibliotheken) für LLM-Schnittstellen, eine große aktive Community sowie eine einfache Lesbarkeit mit sich bringt. \cite*{PythonLanguageAdvantages2017} Die Umsetzung eines Python-Kommandozeilen-Tools erfordert eine detaillierte Anforderungsanalyse, um die Zielfunktionalitäten zu gewährleisten.\\\\
Um die zu verwendende Prompt-Technik und Temperatur festlegen zu können, benötigen zu Beginn eine \textbf{Nutzerabfrage zum Initialisieren der Prompt- und Temperaturvariablen}. Dabei kann zwischen den vorher definierten Promptdesigns gewählt werden. \\ Zur Generierung des Prompts ist es notwendig, dass \textbf{Projekt automatisch zu analysieren und alle relevanten Java Klassen zu erfassen}. Dazu muss das \textit{src}-Verzeichnis genutzt werden, da der relevante Programmcode in diesem abgelegt ist.\\ Sind alle Klassen erfasst, müssen wichtige Details extrahiert werden, sodass dem Sprachmodell ein klar definierter Kontext überliefert wird. Dazu gehören folgende Aspekte:
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item Package
    \item Imports
    \item Name der Klasse
    \item Name der Methode, zu der Tests generiert werden sollen
    \item Konstruktor
    \item Methodeninhalt (Programmcode der Methode)
\end{itemize}
Somit ist es möglich, dem Sprachmodell iterativ alle Methoden in Form eines separaten Prompts mit oben genannten Informationen zu übergeben, um den Fokus und somit den Kontext auf eine einzelne Methode zu setzen. \\ Stehen alle Informationen bereit und wurden korrekt aus der Java-Klasse extrahiert, muss ein \textbf{Prompt erstellt} werden. Der über die Nutzerabfrage gewählte Prompt wird mit den extrahierten Informationen gefüllt und zum Generieren des Testcodes verwendet.\\ Infolgedessen wird eine \textit{Application Programming Interface} (engl. Programmierschnittstelle, kurz API) Anfrage an die vom Anbieter bereitgestellte \acs{API}, zur Kommunikation mit dem LLM, benötigt. Somit lässt sich das \textbf{Generieren von Testcode über eine API-Anfrage} realisieren.\\ Das Ergebnis der Anfrage bzw. die Antwort des Sprachmodells in Form einer Testklasse auf den gestellten Prompt sollte überprüft werden, bevor diese ins Projekt integriert wird. Eine \textbf{Überprüfung der Kompilierbarkeit der Testklasse} sorgt dafür, dass die Tests überhaupt ausführbar sind und bei späterer Durchführung der Prozess aufgrund eines Build- oder Kompilierfehlers nicht scheitert.\\ Treten die eben genannten Fehler auf, sollte dem LLM eine Möglichkeit zur Fehlerbehebung geboten werden. \textbf{Repair Rounds} stellen diesen Aspekt bereit und ermöglichen eine Verbesserung des Testcodes durch die Fehlerbehebung oder Aufforderung, fehlerverursachenden Code oder die komplette Testklasse zu löschen, falls eine Behebung nicht erfolgreich ist. \\ Ist die Kompilierung aller Tests problemlos erfolgt, muss die \textbf{Testklasse in eine Java-Datei geschrieben} werden und im \textit{test}-\textbf{Verzeichnis abgelegt werden}. Dabei soll der komplette Pfad aus dem \textit{src}-Verzeichnis ins \textit{test}-Verzeichnis übernommen werden, um eine übersichtliche Projektstruktur zu gewährleisten.\\\\Zusammengefasst benötigt \textit{Unitcraft} folgende Zielfunktionalitäten:
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item Nutzerabfrage zum Initialisieren der Prompt- und Temperaturvariablen
    \item Automatisches Erfassen aller Java Klassen
    \item Erstellen eines Prompts
    \item Generieren von Testcode über eine API-Anfrage zur Kommunikation mit dem LLM
    \item Überprüfung der Kompilierbarkeit der Testklasse
    \item Repair Rounds zur Fehlerbehebung oder zum Löschen relevanter Codeausschnitte/der Testklasse
    \item Schreiben der Testklasse in Java-Datei und Ablegen im \textit{test}-Verzeichnis mit korrektem Pfad
\end{itemize}
Durch die eben genannten Anforderungen wird eine Evaluation von LLM's hinsichtlich ihrer Eignung zur Generierung von JUnit Tests ermöglicht. Durch Funktionalitäten wie \textit{Repair Rounds} und die Überprüfung der Kompilierbarkeit nähert man sich der Rolle eines manuellen Testers ohne auf den vom Sprachmodell generierten Code Einfluss zu nehmen. Damit wird eine qualitative Analyse gewährleistet.

\section{Wahl des Large Language Models}


\section{Design des Prompts}
Bevor ein Prompt designt werden kann, muss die Prompt-Technik festgelegt werden. Hierbei wird das \textit{Zero-Shot} Prompting sowie \textit{Few-Shot} Prompting in Betracht gezogen. Speziell beim \textit{Few-Shot} Prompting liegt der Fokus auf dem \textit{One-Shot} Prompt, da dieser bei der Generierung von Code Vorteile wie bspw. einem zusätzlichen Beispielkontext mitbringt. Außerdem muss beachtet werden, dass ein Sprachmodell ein begrenztes Kontextfenster in Form von Token besitzt. Somit wäre ein Hinzufügen weiterer Beispielkontexte aufgrund des höheren Tokenverbrauchs für Beispiele ineffektiv und würde die Anzahl an relevanten Codezeilen im Prompt einschränken.\\
Der Aufbau der zu nutzenden Prompts besteht aus verschiedenen Parametern. Das \textit{model} legt fest, an welches Sprachmodell der Prompt gestellt werden soll. Der zweite Parameter ist eine Liste von \textit{messages}, welche an das Modell gesendet werden. Dazu vergibt man pro Nachricht eine \textit{role} sowie einen \textit{content}. Darüber lässt sich die Anweisung an das LLM sowie die verlangte Ausgabe über die \textit{role} ``System'' und die Eingabedaten bzw. ein zusätzlicher Kontext des Users über die \textit{role} ``User'' realisieren. Der Temperaturparameter legt die Kreativität fest und \textit{max tokens} die maximale Anzahl an Tokens, die dem Sprachmodell für den Output zur Verfügung gestellt werden.\\
Insbesondere die Parameter \textit{model}, \textit{temperature} und \textit{max tokens} unterscheiden sich zwischen beiden Prompt-Techniken nicht. Da wir das Sprachmodell GPT-4o nutzen, wird das \textit{model} mit dem String ``gpt-4o'' initialisert. Die Temperatur wird über eine Nutzerabfrage festgelegt, welche den Wert 0, 0.25 und 0.5 vorgibt, da vor allem präzise Aufgaben wie die Codegenerierung eine niedrige Temperatur erfordern um gute Ergebnisse zu erzielen. \cite*{renzeEffectSamplingTemperature2024} Somit wird durch 0 ein deterministischer Wert und 0.5 ein Wert mit Kreativitätsanteil bereitgestellt. Um die Ausgabe nicht einzuschränken setzen wir \textit{max tokens} auf den maximalen Output Wert. Im Falle von GPT-4o ist dieser 4096. Beide Prompt-Techniken erhalten ebenso die in Abbildung \ref{fig:system} dargestellte Systemanweisung.\input{Assets/LaTeX/System.tex}Die Anweisung deckt alle wichtigen Aspekte wie bspw. die Hauptaufgabe, das Ziel, die zu verwendenden Technologien sowie das Ausgabeformat ab. \\
Der \textit{Zero-Shot} Prompt und der \textit{One-Shot} Prompt unterscheiden sich in den bereitgestellten Daten. Die Eingabedaten für den \textit{Zero-Shot} Prompt sieht man in Abbildung \ref{fig:content-0}.\input{Assets/LaTeX/Content-0.tex}Hierbei werden neben einer zusätzlichen Anweisung alle in Kapitel \ref{section:anford} aufgezeigten Informationen der Java-Klasse bereitgestellt. Die geschweiften Klammern werden durch die extrahierten Daten ersetzt. Auch im \textit{One-Shot} Prompt werden die Eingabedaten unter Abbildung \ref{fig:content-0} verwendet. Hinzu kommt jedoch der zusätzliche Beispielkontext wie in Abbildung \ref{fig:content-1} zu erkennen.\input{Assets/LaTeX/Content-1.tex}Die entsprechende Testklasse zum Beispielkontext stellt Quellcode \ref{lst:test-class-context} dar. Dadurch wird beispielhaft gezeigt, wie die bereitgestellten Informationen verarbeitet und verwendet werden können, um eine Testklasse zu erzeugen.\\
\lstinputlisting[caption=\textit{Testklasse für Beispielkontext},captionpos=b,label={lst:test-class-context},language=Java]{Assets/Code/TestClass.java} Bei beiden Prompt-Techniken wird die Test Class der Eingabedaten leer gelassen, sodass dem LLM noch einmal das Generieren einer solchen Klasse signalisiert wird.\\\\
Zur Nutzung von \textit{Repair Rounds} werden jedoch weitere Anweisungen benötigt. Dazu gehört das Beheben von Kompilierfehlern [Abb. \ref{fig:system-repair}] und das Löschen von fehlerhaften Codezeilen [Abb. \ref{fig:system-delete}].\input{Assets/LaTeX/System-Repair.tex}\input{Assets/LaTeX/System-Delete.tex}Beide Anweisungen nennen die Hauptaufgabe und legen den Fokus auf die spezifizierte Anforderung. Es werden ebenso Technologieempfehlungen angesprochen und der Ausgabeindikator festgesetzt. Als Eingabedaten dienen lediglich der generierte Testcode mit dazugehöriger Error-Meldung, was dem \textit{Zero-Shot} Prompting entspricht. Auch die Parameter für \textit{model}, \textit{max tokens} und \textit{temperature} entsprechen denen der vorherigen Prompts.\\\\
Somit wurden 4 verschiedene Prompts designt. Der \textit{Zero-Shot} Prompt sowie \textit{One-Shot} Prompt für das Generieren der Tests und zwei \textit{Zero-Shot} Prompts zum Beheben von Fehlern bzw. Löschen von Codezeilen. 