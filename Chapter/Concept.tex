\chapter{Konzeption}\label{chapter:concept}
Der Inhalt dieses Kapitels stellt die Konzeption eines Testerstellungssystems namens \textit{Unitcraft} zur Bearbeitung des Hauptthemas dar. Dies geschieht in einer nachvollziehbaren Weise, sodass das erstellte Konzept reproduzierbar ist.

\section{Voraussetzungen}
Im Mittelpunkt der Arbeit befindet sich die Programmiersprache Java. Zur Vereinfachung der Analyse und Ausführung der generierten Tests werden die Java-Projekte auf das \textit{Build-Automation-Tool} Apache Maven beschränkt. Hierbei wird die klassische Maven Projektstruktur eingehalten. Diese besteht zunächst aus dem \textit{src}-Verzeichnis, welches sich in das \textit{main}-Verzeichnis, in dem sich der Java-Programmcode bzw. die dazugehörigen Ressourcen befinden, und in das \textit{test}-Verzeichnis unterteilt. \cite{MavenIntroductionStandard} [Abb. \ref{fig:dir}]\\ Somit wird eine klare Trennung zwischen Programmcode und Testcode gewährleistet und deutlich, in welchem Verzeichnis die Tests abgelegt werden müssen. \input{Assets/LaTeX/Dirtree.tex} \newpage
Um ein umfangreiches Generieren von Tests zu ermöglichen, ist die Einbindung von Frameworks als \textit{Dependencies} in der pom.xml essenziell. Neben dem Nutzen von JUnit-Jupiter wird ein Einbinden von Mockito vorausgesetzt, sodass die Verwendung von \textit{mocks} ermöglicht wird.\\
Zur Generierung von Metriken wird SonarQube in das Konzept eingebaut. Dabei wird ein weiteres \textit{Plugin} benötigt, um einen \textit{Coverage-Report} erzeugen zu können. Die \textit{Java Code Coverage Library} (kurz JaCoCo) ermöglicht das Erstellen eines \textit{Code-Reports} und stellt SonarQube alle notwendigen Daten bereit.
Die Implementierung von \textit{Plugins} und \textit{Dependencies} erfolgt im Kapitel \ref{chapter:impl}.

\section{Anforderungsanalyse}\label{section:anford}
Um das Testerstellungssystem praktikabel anwendbar zu gestalten, muss zunächst die Zielgruppe der Arbeit betrachtet werden. Es handelt sich dabei um Personen, die fachlich zugeordnet sind, und somit eine Wissensbasis im Bereich der Informatik besitzen. Aufgrund dessen wird auf eine nutzerfreundliche Oberfläche verzichtet und somit der Fokus auf die Programmlogik gelenkt.\\ Ein Testerstellungssystem als Kommandozeilen-Tool bietet hier eine vereinfachte Anwendung innerhalb des Projekts und eröffnet die Möglichkeit einer zukünftigen Einbindung in Automationsprozesse wie bspw. der \textit{Continuos Integration} (engl. kurz CI).\\ Die Wahl der Programmiersprache fällt hierbei auf Python. Python ist eine \textit{high-level}, interpretierte und dynamische Programmiersprache, welche Vorteile wie bspw. zahlreiche \textit{Libraries} für LLM-Schnittstellen, eine große aktive Community sowie eine einfache Lesbarkeit mit sich bringt. \cite*{PythonLanguageAdvantages2017} Die Umsetzung eines Python-Kommandozeilen-Tools erfordert eine detaillierte Anforderungsanalyse, um die Zielfunktionalitäten zu gewährleisten.\\\\
Um die zu verwendende Prompt-Technik und Temperatur festlegen zu können, wird zu Beginn eine \textbf{Nutzerabfrage zum Initialisieren der Prompt- und Temperaturvariablen} benötigt. Dabei kann zwischen den vorher definierten Promptdesigns gewählt werden. \\ Zur Generierung des Prompts ist es notwendig, das \textbf{Projekt automatisch zu analysieren und alle relevanten Java Klassen zu erfassen}. Dazu muss das \textit{src}-Verzeichnis genutzt werden, da der relevante Programmcode in diesem abgelegt ist.\\ Sind alle Klassen erfasst, müssen wichtige Details extrahiert werden, sodass dem Sprachmodell ein klar definierter Kontext überliefert wird. Dazu gehören folgende Aspekte:
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item \textit{Package}
    \item \textit{Imports}
    \item Name der Klasse
    \item Name der Methode, zu der Tests generiert werden sollen
    \item Konstruktor
    \item Methodeninhalt (Programmcode der Methode)
\end{itemize}
Somit ist es möglich, dem Sprachmodell iterativ alle Methoden in Form eines separaten Prompts mit oben genannten Informationen zu übergeben, um den Fokus und somit den Kontext auf eine einzelne Methode zu setzen. \\ Stehen alle Informationen bereit und wurden korrekt aus der Java-Klasse extrahiert, muss ein \textbf{Prompt erstellt} werden. Der über die Nutzerabfrage gewählte Prompt wird mit den extrahierten Informationen gefüllt und zum Generieren des Testcodes verwendet.\\ Infolgedessen wird eine \textit{Application Programming Interface} (engl. kurz API) Anfrage an die vom Anbieter bereitgestellte \acs{API}, zur Kommunikation mit dem LLM, benötigt. Somit lässt sich das \textbf{Generieren von Testcode über eine API-Anfrage} realisieren.\\ Das Ergebnis der Anfrage bzw. die Antwort des Sprachmodells in Form einer Testklasse auf den gestellten Prompt sollte überprüft werden, bevor diese ins Projekt integriert wird. Eine \textbf{Überprüfung der Kompilierbarkeit der Testklasse} sorgt dafür, dass die Tests überhaupt ausführbar sind und bei späterer Durchführung der Prozess aufgrund eines \textit{Build-} oder Kompilierfehlers nicht scheitert.\\ Treten die eben genannten Fehler auf, sollte dem LLM eine Möglichkeit zur Fehlerbehebung geboten werden. \textbf{Repair Rounds} stellen diesen Aspekt bereit und ermöglichen eine Verbesserung des Testcodes durch die Fehlerbehebung oder Aufforderung, fehlerverursachenden Code oder die komplette Testklasse zu löschen, falls eine Behebung nicht erfolgreich ist. \\ Ist die Kompilierung aller Tests problemlos erfolgt, muss die \textbf{Testklasse in eine Java-Datei geschrieben} und im \textit{test}-\textbf{Verzeichnis abgelegt werden}. Dabei soll der komplette Pfad aus dem \textit{src}-Verzeichnis ins \textit{test}-Verzeichnis übernommen werden, um eine übersichtliche Projektstruktur zu gewährleisten.\\\\Zusammengefasst benötigt \textit{Unitcraft} folgende Zielfunktionalitäten:
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item Nutzerabfrage zum Initialisieren der Prompt- und Temperaturvariablen
    \item Automatisches Erfassen aller Java Klassen
    \item Erstellen eines Prompts
    \item Generieren von Testcode über eine API-Anfrage zur Kommunikation mit dem LLM
    \item Überprüfung der Kompilierbarkeit der Testklasse
    \item \textit{Repair Rounds} zur Fehlerbehebung oder zum Löschen relevanter Codeausschnitte/der Testklasse
    \item Schreiben der Testklasse in Java-Datei und Ablegen im \textit{test}-Verzeichnis mit korrektem Pfad
\end{itemize}
Durch die eben genannten Anforderungen wird eine Evaluation von LLM's hinsichtlich ihrer Eignung zur Generierung von JUnit Tests ermöglicht. Durch Funktionalitäten wie \textit{Repair Rounds} und die Überprüfung der Kompilierbarkeit nähert man sich der Rolle eines manuellen Testers, ohne auf den vom Sprachmodell generierten Code Einfluss zu nehmen. Damit wird eine qualitative Analyse gewährleistet.

\section{Wahl des Large Language Models}
Bereits in Kapitel \ref{subsection:gpt} wurde erklärt, dass die Serie der GPT, speziell die Sprachmodelle von OpenAI, aufgrund ihrer hohen Performance hervorstechen. Vergleicht man die aktuellsten Modelle miteinander, wird deutlich, dass GPT-4o vor allem im \textit{Massive Multi-task Language Understanding} (kurz MMLU) am besten abschneidet. \acs{MMLU} ist ein \textit{Benchmark} für NLP-Modelle, mithilfe dessen sich Fähigkeiten zum Lösen verschiedener Aufgabenstellungen beurteilen lassen. Mit einem Wert von 88.7\% erreicht das GPT-4o Modell die beste Beurteilung. [Abb. \ref{fig:gpt-comp}] \cite{HelloGPT4o} Ebenso Aspekte wie die Aktualität, die verbesserte Geschwindigkeit und der halb so hohe Preis im Vergleich zum aktuellen GPT-4 Modell sprechen für die Verwendung von GPT-4o. \cite{HelloGPT4o} \input{Assets/LaTeX/GPT-Comp.tex}

\section{Design des Prompts}
Bevor ein Prompt designt werden kann, muss die Prompt-Technik festgelegt werden. Hierbei wird das \textit{Zero-Shot} Prompting sowie \textit{Few-Shot} Prompting in Betracht gezogen. Speziell beim \textit{Few-Shot} Prompting liegt der Fokus auf dem \textit{One-Shot} Prompt, da dieser bei der Generierung von Code Vorteile wie bspw. einem zusätzlichen Beispielkontext mitbringt. Außerdem muss beachtet werden, dass ein Sprachmodell ein begrenztes Kontextfenster in Form von Token besitzt. Somit wäre ein Hinzufügen weiterer Beispielkontexte aufgrund des höheren Tokenverbrauchs für Beispiele ineffektiv und würde die Anzahl an relevanten Codezeilen im Prompt einschränken.\\
Der Aufbau der zu nutzenden Prompts besteht aus verschiedenen Parametern. Das \textit{model} legt fest, an welches Sprachmodell der Prompt gestellt werden soll. Der zweite Parameter ist eine Liste von \textit{messages}, welche an das Modell gesendet werden. Dazu vergibt man pro Nachricht eine \textit{role} sowie einen \textit{content}. Darüber lässt sich die Anweisung an das LLM sowie die verlangte Ausgabe über die \textit{role} ``System'' und die Eingabedaten bzw. ein zusätzlicher Kontext des Users über die \textit{role} ``User'' realisieren. Der Temperaturparameter legt die Kreativität fest und \textit{max tokens} die maximale Anzahl an Tokens, die dem Sprachmodell für den Output zur Verfügung gestellt werden.\\
Insbesondere die Parameter \textit{model}, \textit{temperature} und \textit{max tokens} unterscheiden sich zwischen beiden Prompt-Techniken nicht. Da wir das Sprachmodell GPT-4o nutzen, wird das \textit{model} mit dem String ``gpt-4o'' initialisiert. Die Temperatur wird über eine Nutzerabfrage festgelegt, welche den Wert 0, 0.25 und 0.5 vorgibt, da vor allem präzise Aufgaben wie die Codegenerierung eine niedrige Temperatur erfordern, um gute Ergebnisse zu erzielen. \cite*{renzeEffectSamplingTemperature2024} Somit wird durch 0 ein deterministischer Wert und 0.5 ein Wert mit Kreativitätsanteil bereitgestellt. Um die Ausgabe nicht einzuschränken setzen wir \textit{max tokens} auf den maximalen Output Wert. Im Falle von GPT-4o ist dieser 4096 Token. Beide Prompt-Techniken erhalten ebenso die in Abbildung \ref{fig:system} dargestellte Systemanweisung.\input{Assets/LaTeX/System.tex}Die Anweisung deckt alle wichtigen Aspekte wie bspw. die Hauptaufgabe, das Ziel, die zu verwendenden Technologien sowie das Ausgabeformat ab. \\
Der \textit{Zero-Shot} Prompt und der \textit{One-Shot} Prompt unterscheiden sich in den bereitgestellten Daten. Die Eingabedaten für den \textit{Zero-Shot} Prompt sieht man in Abbildung \ref{fig:content-0}.\input{Assets/LaTeX/Content-0.tex}Hierbei werden neben einer zusätzlichen Anweisung alle in Kapitel \ref{section:anford} aufgezeigten Informationen der Java-Klasse bereitgestellt. Die geschweiften Klammern werden durch die extrahierten Daten ersetzt. Auch im \textit{One-Shot} Prompt werden die Eingabedaten unter Abbildung \ref{fig:content-0} verwendet. Hinzu kommt jedoch der zusätzliche Beispielkontext wie in Abbildung \ref{fig:content-1} zu erkennen.\input{Assets/LaTeX/Content-1.tex}Die entsprechende Testklasse zum Beispielkontext stellt Quellcode \ref{lst:test-class-context} dar.\\
\lstinputlisting[caption=Testklasse für Beispielkontext,captionpos=b,label={lst:test-class-context},language=Java]{Assets/Code/TestClass.java}\vspace{-.3cm} Dadurch wird beispielhaft gezeigt, wie die bereitgestellten Informationen verarbeitet und verwendet werden können, um eine Testklasse zu erzeugen. Bei beiden Prompt-Techniken wird die Test Klasse der Eingabedaten leer gelassen, sodass dem LLM noch einmal das Generieren einer solchen Klasse signalisiert wird.\\
Zur Nutzung von \textit{Repair Rounds} werden jedoch weitere Anweisungen benötigt. Dazu gehört das Beheben von Kompilierfehlern [Abb. \ref{fig:system-repair}] und das Löschen von fehlerhaften Codezeilen [Abb. \ref{fig:system-delete}].\input{Assets/LaTeX/System-Repair.tex}\input{Assets/LaTeX/System-Delete.tex}Beide Anweisungen nennen die Hauptaufgabe und legen den Fokus auf die spezifizierte Anforderung. Es werden ebenso Technologieempfehlungen angesprochen und der Ausgabeindikator festgesetzt. Als Eingabedaten dienen lediglich der generierte Testcode mit dazugehöriger Error-Meldung, was dem \textit{Zero-Shot} Prompting entspricht. Auch die Parameter für \textit{model}, \textit{max tokens} und \textit{temperature} entsprechen denen der vorherigen Prompts.\\
Somit wurden 4 verschiedene Prompts designt. Der \textit{Zero-Shot} Prompt sowie \textit{One-Shot} Prompt für das Generieren der Tests und zwei \textit{Zero-Shot} Prompts zum Beheben von Fehlern bzw. Löschen von Codezeilen. 

\section{Programmablaufplan}
Um einen qualitativen Testgenerierungsprozess zu gewährleisten, soll \textit{Unitcraft} nach dem in Abbildung \ref{fig:pap} gezeigten Programmablaufplan erstellt werden. Dieser beschreibt die Folge an Operationen die für die Generierung der Tests und das Erreichen der Zielfunktionalitäten in Kapitel \ref{section:anford} notwendig sind.\input{Assets/LaTeX/PAP.tex}Zunächst findet über die Nutzerabfrage eine Initialisierung der Temperatur und Prompt-Technik statt. Im Anschluss wird das Projekt analysiert und alle Java-Dateien aus dem \textit{src}-Verzeichnis erfasst. Ist dieses Verzeichnis leer oder es existieren keine Java-Dateien, endet das Programm. Im anderen Falle werden die Informationen aller Java-Dateien iterativ extrahiert sowie die entsprechenden Testklassen generiert.\\Der Prozess der Testgenerierung startet mit einer API-Anfrage an die OpenAI-API, in dem mithilfe erfasster Parameter ein Prompt gestellt wird. Sollte die Antwort vom Sprachmodell kompilierbar sein, so wird die Testklasse als Java-Datei im Testpfad abgelegt und das Programm fährt mit der nächsten Java-Datei fort, insofern diese existiert. Andernfalls endet das Programm an dieser Stelle.\\Tritt ein Kompilierfehler o.Ä. auf, startet der Prozess der \textit{Repair Rounds}. Dieser beginnt ebenso mit einer API-Anfrage an die OpenAI-API, in welcher der Prompt zur Reparatur genutzt wird. Sollte die Antwort kompilierbar sein, so wird die Testklasse als Java-Datei im Testpfad abgelegt. Tritt erneut ein Kompilierfehler auf erfolgt eine neue \textit{Repair Round}. Insgesamt gibt es die Möglichkeit auf 2 Runden.\\Ist diese erschöpft, erfolgt eine letzte API-Anfrage an die OpenAI-API zum Löschen der fehlerhaften Codezeilen, sodass der funktionierende Restcode nicht verloren geht.\\Tritt auch danach wieder ein Kompilierfehler auf, wird die komplette Testklasse von \textit{Unitcraft} gelöscht und der Prozess fährt bei der nächsten Java-Datei fort bzw. das Programm endet, falls keine Java-Datei mehr vorhanden ist.