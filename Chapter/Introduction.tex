%!TeX root = ./../Thesis.tex

%##########################################################
% Inhalt
%##########################################################
\pagenumbering{arabic}
\chapter{Einleitung}
In den letzten Jahren haben \acs{KI}-Technologien in nahezu allen Bereichen des Lebens an Bedeutung gewonnen. Besonders generative KI-Modelle wie Chat\acs{GPT} demonstrieren ihr Potenzial und sind heutzutage aus der Softwareentwicklung nicht mehr wegzudenken. \cite{hammermann_ki_nodate} Durch die Vielzahl an Automatisierungsmöglichkeiten im Entwicklungsprozess, kann KI zur Effizienzsteigerung und Kosteneinsparung beitragen.\\ Der Softwareentwicklungszyklus (engl. kurz \acs{SDLC}), ein weit verbreiteter Ansatz, unterteilt den Entwicklungsprozess in einzelne Phasen, was den gezielten Einsatz von KI zur Unterstützung spezifischer Aufgaben ermöglicht.\\ Insbesondere das Testen von Software, als eine dieser Phasen im SDLC, spielt eine zentrale Rolle bei der Sicherung von Codequalität, dem Vorbeugen von Fehlern und der Überprüfung von Anforderungen. \cite{noauthor_was_nodate} Durch den Einsatz von \textit{Large Language Models} (engl. Große Sprachmodelle, kurz \acs{LLM}) erhofft man sich im Testprozess eine Entlastung der Entwickler und somit eine Produktivitätssteigerung in der Entwicklung.

\section{Problemstellung}
Das Testen von Software ist ein wichtiger, unverzichtbarer Bestandteil der Softwareentwicklung. Frühes und qualitatives Testen stellt sicher, dass Fehler bei Ihrer Entstehung erkannt und somit Kosten und Aufwand gespart werden. Dies bringt jedoch Nachteile mit sich, da umfassende Tests ressourcen- und zeitaufwendig sein können. Sie können Tester erschöpfen und viel Zeit in Anspruch nehmen, da sie oft manuell durchgeführt und bis zur Erreichung eines fehlerfreien Zustands mehrfach wiederholt werden müssen. \cite{pargaonkar_study_2023}\\ Somit rückt der Einsatz von LLM's immer mehr in den Fokus, um die genannten Nachteile zu kompensieren und die Tester zu unterstützen. KI-Tools wie GitHub-Copilot und ChatGPT bieten ein enormes Potenzial bei der Generierung von Code und assistieren dem Tester bei seiner Arbeit. Beide Systeme basieren auf einem LLM von OpenAI und fungieren als Chatbots, die auf bestimmte Eingabeaufforderungen (\textit{Prompts}) antworten. Analysen zeigen, dass diese Systeme nicht vollständig zuverlässig sind, da sie bei komplexen Anforderungen Unsicherheiten aufweisen. \cite{poldrack_ai-assisted_2023} Aus diesem Grund ist eine manuelle Nachbearbeitung essenziell. Wie sich LLM's verhalten, wenn sie die Rolle eines Testers übernehmen, ist abhängig von einer Vielzahl von Parameterkombinationen und bietet somit einen umfangreichen Forschungsraum. Es stellt sich die Frage, ob der gewünschte Vorteil der Arbeitsentlastung oder gar Ersetzung tatsächlich eintritt.\\\\ Um ein LLM der Rolle des Testers so nah wie möglich zu bringen, bedarf es der Einbindung in ein System, welches ein automatisiertes Generieren von Tests, ohne manuellen Einfluss, ermöglicht. Einer Umfrage zur Folge, ist das Testen von Units \textit{(Unit Testing)} eine der grundlegendsten Methoden, die ebenfalls die größte Aufmerksamkeit von Entwicklern erhält. \cite{garousi_survey_2013} Vor allem im Kontext Java-basierter Anwendungen bedarf es der Entwicklung eines zuverlässigen Systems, zur automatisierten Generierung dieser Tests, auf Basis eines LLM's. Ein solches System eröffnet die Möglichkeit, Sprachmodelle hinsichtlich ihrer Eignung zur automatisierten Generierung von Unit-Tests für Java-Anwendungen im Vergleich zu manuellen Testern zu evaluieren.\\ Dabei ist es wichtig zu berücksichtigen, dass das Design des \textit{Prompts} einen maßgeblichen Einfluss auf die zu erzielenden Ergebnisse ausübt. Ein Experimentieren mit verschiedenen Designs ist somit erforderlich, um die Vollständigkeit eines solchen Systems zu gewährleisten.


\section{Zielstellung} 
Zu Beginn der Arbeit werden relevante technische Grundlagen geschaffen, welche im Verlauf der Arbeit essenziell sind, um die zur Lösung der Problemstellung beitragenden Technologien und Konzepte zu verstehen.\\\\ Um die Sprachmodelle effizient nutzen zu können, müssen zunächst verschiedene \textit{Prompts} designt und anschließend analysiert werden. Dabei soll der Fokus auf grundlegenden \textit{Prompt}-Techniken wie bspw. \textit{Zero-Shot-} und \textit{Few-Shot-Prompting} liegen. Ein weiterer Bestandteil des \textit{Prompt Engineering} sind LLM-Parameter, welche die Ergebnisse des \textit{Outputs} (engl. Ausgabe) beeinflussen. Damit ist es essenziell eine Kombination aus verschieden gesetzten LLM-Parametern und Promptdesigns zu analysieren.\\\\ Die Basis des Ganzen bildet ein Testerstellungssystem, das ein Sprachmodell integriert, um Unit-Tests zu erzeugen. Hierbei soll die Programmiersprache Python genutzt werden, um ein zuverlässiges Programmgerüst um das Sprachmodell zu bauen. 
Um sich der Rolle des Testers anzunähern, werden Funktionen wie \textit{Repair Rounds} (engl. Reparatur Runden) implementiert. Dadurch hat das Modell die Möglichkeit, sich wie ein echter Softwaretester zu verbessern. Wichtig ist, nicht in die Ergebnisse des Modells einzugreifen, sodass eine klare Bewertung der Qualität möglich ist.\\\\ Zu einer hochwertigen Qualitätsanalyse gehört ein festes Bewertungsverfahren sowie eine aussagekräftige Anzahl an generierten Tests. Neben einer Auswahl von Projekten mit unterschiedlichen Komplexitätsgraden, ist es erforderlich, im Vorfeld klare und aussagekräftige Metriken zu definieren, die bei allen Projekten verwendet werden.\\\\ Die Erfüllung aller genannten Ziele bildet die Grundlage für eine aussagekräftige Analyse, welche die Qualität misst, mit der ein LLM automatisiert und ohne Einfluss manueller Faktoren Tests erstellen kann. Dadurch lässt sich bewerten, in welchem Maße Sprachmodelle Tester ersetzen können und in welcher Qualität sie deren Arbeit übernehmen.

\section{Methodik}
