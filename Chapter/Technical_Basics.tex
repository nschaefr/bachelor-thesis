%!TeX root = ./../Thesis.tex

%##########################################################
% Inhalt
%##########################################################
\chapter{Grundlagen}
Dieses Kapitel ist zur Vermittlung von technischen Grundlagen, um im Verlauf der Arbeit angewandte Technologien zu verstehen und umgesetzte Lösungsversuche nachvollziehen zu können. Dabei wird Fokus auf den SDLC, das Thema der LLM's sowie dem \textit{Prompt Engineering} gelegt. Da Unit-Tests einen wichtigen Bestandteil der Arbeit repräsentieren, werden Grundlagen zu diesen Tests geschaffen und der Bezug zur Programmiersprache Java hergestellt.

\section{Software Development Lifecycle}
Der SDLC ist ein Grundlagenzyklus, welcher innerhalb eines Softwareprojektes festgelegt und während der Bearbeitung des Projektes befolgt wird. \cite*{SoftwareDevelopmentLife} Dieser dient zur Verbesserung von Qualität und Entwicklung.\input{Assets/LaTeX/SDLC.tex}Ein typischer SDLC besteht aus 6 verschiedenen Phasen. [Abb. \ref{fig:sdlc}]\\\\
\textbf{Planung}\\[0.2cm]
Eine Planung erfolgt in der Regel durch eine neue Innovation, welche von einem Endnutzer oder Sponsor ausgeht. \cite*{tikySoftwareDevelopmentLife} Hierbei werden Projektumfang, Ziele sowie Anforderungen definiert. Dabei steht vor allem der Projektplan im Fokus, welcher Zeitrahmen und Ressourcen definiert. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Entwurf}\\[0.2cm]
In der Entwurfsphase wird festgelegt, wie benötigte Softwarekomponenten zusammenwirken und eine Produktarchitektur definiert. Es kommt zum Entwerfen des \textit{User Interfaces} (engl. Benutzeroberfläche) mit Blick auf die \textit{User Experience} (engl. Benutzererfahrung). \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Entwicklung und Implementierung}\\[0.2cm]
Nach Abschluss aller vorher nötigen Phasen, beginnen Programmierer mit der Entwicklung und Implementation der Software bei Einhaltung aller festgelegten Anforderungen und entsprechend dem Architekturentwurf. \cite*{tikySoftwareDevelopmentLife} \\\\
\textbf{Testen}\\[0.2cm]
Das Testen der Software ist eine wichtige und essenzielle Phase, die während sowie nach der Entwicklung stattfindet. Hierbei kommt es zur Durchführung verschiedener Teststufen, einschließlich Unit-Tests, Integration Tests, System Tests und Benutzerakzeptanz Tests. Es werden Fehler identifiziert und behoben sowie Anforderungen aus vorherigen Phasen überprüft. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Deployment und Wartung}\\[0.2cm]
Zum \textit{Deployment} gehört die Bereitstellung der Software in einer Staging-Umgebung. Diese wird vorausgesetzt, um abschließende Tests durchzuführen und die Software zu validieren. Es werden Server und Datenbanken konfiguriert, sodass eine Produktionsumgebung vorbereitet wird. Nach erfolgreichen \textit{Rollout} (engl. ausrollen) der Software steht die Überwachung und Wartung der Produktionsumgebung im Fokus. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
Insbesondere die Phase des Testens sollte als eine \textit{Sub stage} (engl. Unterstufe) jeder Phasen betrachtet werden, da ein Testen von Anforderungen, Implementation, Code sowie Deployment dauerhaft notwendig ist. Aufgrund dessen ist die Testphase besonders komplex und aufwändig, weshalb der Fokus ständig aufrechterhalten werden muss. \cite*{tikySoftwareDevelopmentLife}

\section{Large Language Models}
\subsection{Begriff und Einführung}
Hinter dem simplen Begriff \textit{Large Language Model} (engl. Großes Sprachmodell) steckt eine komplizierte Technologie mit großer Herkunftsgeschichte, welche eine Definition mehrerer Begrifflichkeiten im Feld der Künstlichen Intelligenz (kurz KI) voraussetzt. Dadurch kann ein Grundverständnis geschaffen werden, um den Begriff LLM besser einordnen zu können. \cite{seemannKuenstlicheIntelligenzLarge} \\\\ Ein Hauptbestandteil der KI spielen \textbf{Künstliche Neuronale Netze} (kurz \acs{KNN}). 
Die Inspiration dieser Netze entstammt dem Neuronalen Netzwerk im Gehirn eines Menschen. \cite*{seemannKuenstlicheIntelligenzLarge} Das Ziel dabei ist, natürliche Organismen nachzubauen, und somit lernfähige Programme zu erzeugen. 
\input{Assets/LaTeX/KNN}Der Aufbau eines solchen Netzes folgt dem Vorbild der Struktur des menschlichen Gehirns und besteht somit aus verbundenen künstlichen Nervenzellen (Neuronen), welche durch eine Eingabeschicht passende Ausgaben erzeugen. \cite*{morrKuenstlicheNeuronaleNetze} Unterschieden wird zwischen 3 verschiedenen Arten von Neuronen. Somit besteht ein Netz grundlegend aus Input-, Hidden- und Outputneuronen, zwischen denen eine gewichtete Verbindung herrschen kann. \cite*{NeuronaleNetzeEinfuhrung} [Abb. \ref{fig:knn}] Durch solch eine Struktur können KNN durch maschinelles Lernen mit großen Datenmengen trainiert werden und dadurch Fähigkeiten wie das Generieren von Texten erlernen. \cite*{seemannKuenstlicheIntelligenzLarge}\\\\ Ein weiteres Teilgebiet in der KI ist das \textbf{Natural Language Processing} (engl. natürliche Sprachverarbeitung, kurz NLP). Hierbei liegt der Fokus auf der Interaktion zwischen Computern und menschlicher Sprache, in dem Algorithmen und Modelle ermöglichen, dass Computer die menschliche Sprache verstehen, interpretieren und auf sinnvolle Weise erzeugen können. \cite*{muruganNaturalLanguageProcessing2024} \\\\ Dies schafft die Grundlage eines \textbf{Large Language Models}, denn diese Modelle basieren auf KNN und sind spezialisiert auf die natürliche Sprachverarbeitung. \cite*{seemannKuenstlicheIntelligenzLarge} Sie sind ausgelegt, um Beziehungen zwischen einzelnen Daten zu lernen und Sequenzen vorherzusagen. Insbesondere deswegen sind sie für die Generierung von Text in natürlicher Sprache geeignet, da sie die Fähigkeit besitzen Sprache aus großen Datensätzen von geschriebenen Texten im Internet sowie digitalisierten Büchern zu erlernen, um sich in neuartigen Situationen angepasst verhalten zu können. \cite*{okerlundWhatChatterbox2022} \cite*{authorityofthehouseoflordsLargeLanguageModels} \\Ein \textit{pre-training} (engl. Vortraining) sowie \textit{fine-tuning} (engl. Feinabstimmung) ist in vielen Fällen notwendig, um die Genauigkeit des Modells auf bestimmte Bereiche zu verbessern. \cite{HowLLMsWork} Um eine solche Genauigkeit zu gewährleisten, arbeiten Sprachmodelle mit dem Prinzip der \textit{Tokenisierung}. Im Rahmen dieses Vorverarbeitungsschritts erfolgt eine Zerlegung des Textes in Token, wobei diese aus Zeichen, Teilwörtern oder Symbolen bestehen können. \cite*{naveedComprehensiveOverviewLarge2024} \\ Grundlegend nehmen LLM's einen Prompt als Eingabe entgegen und generieren abhängig von gesetzten Parametern des Modells, sowie bisher erlernten Informationen, eine Ausgabe. \input{Assets/LaTeX/Word_process.tex}Das vorhergesagte Wort ist das Ergebnis einer Wahrscheinlichkeitsverteilung des Modells und wird so lange mit der Sequenz verknüpft, bis sie endgültig ist. [Abb. \ref{fig:word}] Dabei können unterschiedliche Ergebnisse erzielt werden, welche abhängig von der \textit{sampling strategy} (engl. Stichprobenstrategie) sind. \cite*{cholletDeepLearningPython2018} Ein Beispiel wäre \textit{sampling with temperature} (engl. Stichprobeverfahren mit Temperatur-Parameter), bei dem die Wahl der Temperatur entscheidungsgebend ist. Wählt man bspw. einen Wert nahe 0, so wird das Sprachmodell in den meisten Fällen das Ergebnis mit der höchsten Wahrscheinlichkeit nutzen. Umso höher der Wert, desto eher wird das Ergebnis mit einer niedrigeren Wahrscheinlichkeit genutzt. \cite*{WeightsBiases} Dies bedeutet, um auf die Qualität der Ergebnisse Einfluss zu nehmen, gibt es die Möglichkeit manuelle Parameter zu setzen. Darunter zählen z.B. \textit{max tokens} zum Festlegen der maximalen Anzahl an Tokens, die generiert werden dürfen oder auch, wie oben erwähnt, die \textit{temperature}, welche die Zufälligkeit von 0.0 (deterministisch) - 2.0 (extrem zufällig) regelt.\\ Im Allgemeinen finden LLM's eine große Spannbreite an Anwendung, wie bspw. Textgenerierung, Vervollständigung und Zusammenfassung. \cite*{dhadukHowLargeLanguage2023}

\subsection{Generative Pre-trained Transformer}
Unter allen Sprachmodellen, wie bspw. FLAN oder PaLM, sticht die Serie der \textit{Generative Pre-trained Transformer} (engl. vortrainierter generativer Transformator, kurz GPT) durch ihre besonders hohe Performance hervor. \cite*{yeComprehensiveCapabilityAnalysis2023} Die in GPT verwendete Transformer-Architektur ist ein bedeutender Fortschritt gegenüber früheren \acs{NLP}-Ansätzen. Durch ihren Mechanismus der Selbstbeobachtung ermöglicht sie es dem Modell, bei der Wortgenerierung, den Kontext des gesamten Satzes zu berücksichtigen. \cite*{yenduriGenerativePretrainedTransformer2023} \\ OpenAI's Sprachmodelle sind Teil dieser Serie und haben mit GPT-3.5-turbo sowie GPT-4-turbo zwei der populärsten LLM's auf den Markt gebracht. \cite*{yaoSurveyLargeLanguage2024}\\\\
\textbf{GPT-3.5-turbo}\\[0.2cm]
GPT-3.5-turbo basiert auf GPT-3 und wurde von OpenAI mit 175 Milliarden \textit{Machine Learning} Parametern entwickelt, um natürliche Sprache zu generieren. Aktuell stellt es das am weitesten fortgeschrittene Modell innerhalb seiner Serie dar. \cite*{yeComprehensiveCapabilityAnalysis2023} Es ist das schnellste und kostengünstigste Modell von OpenAI, das auf einem großen Datensatz aus dem Internet, der Daten bis September 2021 einschließt, trainiert wurde. Die aktuellste Version besitzt ein Kontext-Fenster von 16.385 Token und hat einen maximalen \textit{Output} von 4.096 Token. \cite*{OpenAIPlatform}\\\\
\textbf{GPT-4-turbo}\\[0.2cm]
GPT-4-turbo ist eine verbesserte Version des GPT-4 Modells, aufgrund einer höheren Anzahl von Parametern sowie der verbesserten Effizienz bei der Generierung von Antworten und findet vermehrt Einsatz im populären Chatbot \textit{ChatGPT}. \cite*{shahandashtiEvaluatingEffectivenessGPT42024} Es besitzt ein Kontext-Fenster von 128.000 Token und ermöglicht einen \textit{Output} von 4.096 Token. \cite*{OpenAIPlatform}\\\\
Beide Modelle erreichen im Vegleich mit anderen Modellen die höchste Performance bei der Codegenerierung und treffen somit auf eine hohe Anwendung in diesem Bereich. \cite*{duEvaluatingLargeLanguage2024} Tabelle \ref{fig:gpt} zeigt die wichtigsten Vergleiche zwischen beiden Modellen.
\input{Assets/LaTeX/GPT-Table.tex}


\section{Prompt Engineering}
Das \textit{Prompt-Engineering} spiegelt eine Technik zur Verbesserung der Fähigkeiten von LLM's wieder. Es werden sogenannte \textit{Prompts} genutzt, welche eine strategische und spezifische Anweisung repräsentieren, um die Modellausgabe zu steuern. Somit wird ein Mechanismus bereitgestellt, der ein \textit{fine tuning} des \textit{Outputs} ermöglicht und das Modell anpassungsfähiger macht. \cite*{sahooSystematicSurveyPrompt2024} \input{Assets/LaTeX/Prompt-Eng.tex}

\subsection{Prompt Elemente}
Der Aufbau eines Promptes besteht aus folgenden Elementen [Abb. \ref{fig:prompt-eng}]:
\begin{itemize}
    \item \textbf{Anweisung} - eine Anweisung, die das Modell ausführen soll
    \item \textbf{Kontext} - Zusatzkontext, der das Modell zu besseren Antworten lenken kann
    \item \textbf{Eingabedaten} - die Eingabe, für die eine Antwort gefunden werden soll
    \item \textbf{Ausgabeindikator} - das Format der Ausgabe
\end{itemize}
Es sind nicht alle Elemente nötig um einen Prompt zu kreieren, bspw. benötigt es keinen Kontext, wenn die Eingabedaten für genug Klarheit schaffen. \cite*{ElementsPromptNextra2024}

\subsection{Prompt Techniken}
Wenn Aufgaben an Größe und Komplexität zunehmen, erzielt ein einfacher \textit{Prompt} in den meisten Fällen nicht das gewünschte Ergebnis. Zur Behebung dieses Problems gibt es verschiedene \textit{Prompt Techniken}, die zu mehr Zuverlässigkeit und Leistung von LLM's führen. Die gewöhnlichsten und zugleich effektivsten Techniken sind der Zero-Shot Prompt sowie Few-Shot Prompt. \cite*{PromptEngineeringGuide}\\\\
\textbf{Zero-Shot Prompt}\\[0.2cm]
Wie in vorherigen Kapiteln beschrieben, werden Sprachmodelle wie GPT-3.5-turbo oder GPT-4-turbo anhand großer Datenmengen trainiert und sind auf die Befolgung von Anweisungen abgestimmt. Dadurch sind sie in der Lage Aufgaben anhand eines \textit{Prompts} zu lösen, welcher keine Beispiele oder Demonstrationen enthält. Dies bedeutet, dass der \textit{Zero-Shot Prompt} das Modell anweist, ohne zusätzliche Beispiele zur Steuerung anzugeben. Ein Beispiel ist in Abbildung \ref{fig:prompt-zero} zu sehen.\input{Assets/LaTeX/Prompt-Zero.tex}Bei komplexeren Anforderungen kann es dazu kommen, dass ein \textit{Zero-Shot Prompt} nicht funktionert bzw. nicht zum erwarteten Ergebnis führt. In diesem Fall empfiehlt es sich, im \textit{Prompt} zusätzlichen Kontext in Form von Demonstrationen oder Beispielen hinzuzufügen. \cite{PromptEngineeringGuide}\\\\
\textbf{Few-Shot Prompt}\\[0.2cm]
\textit{Few-Shot Prompting} wird als Technik verwendet, um kontextbezogenes Lernen durch Demonstrationen und Beispiele, innerhalb des \textit{Prompts}, zu ermöglichen und dadurch das Modell zu einer besseren Leistung zu führen. Hierbei eröffnet sich die Möglichkeit, die Lösung der Aufgabe in eine gewünschte bzw. bestimmte Richtung zu steuern. Abbildung \ref{fig:prompt-one} zeigt ein Beispiel für einen \textit{Few-Shot Prompt}. \input{Assets/LaTeX/Prompt-One.tex}\\
Zusammenfassend lässt sich sagen, dass \textit{Prompt Engineering} Anweisungen nutzt, die ein \textit{fine tuning} der Leistung von vortrainierten LLM's basierend auf spezifischen \textit{Prompts} ermöglicht, und somit die Vielseitigkeit und Anwendung in unterschiedlichen Bereichen verbessert. \cite*{sahooSystematicSurveyPrompt2024}

\section{Unit Testing}


