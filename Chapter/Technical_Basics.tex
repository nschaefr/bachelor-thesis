%!TeX root = ./../Thesis.tex

%##########################################################
% Inhalt
%##########################################################
\chapter{Grundlagen}
Dieses Kapitel ist zur Vermittlung von technischen Grundlagen, um im Verlauf der Arbeit angewandte Technologien zu verstehen und umgesetzte Lösungsversuche nachvollziehen zu können. Dabei wird Fokus auf das Thema der LLM's, dem \textit{Prompt Engineering} sowie SonarQube gelegt. Der SDLC wird noch einmal genauer erläutert, um die Einordnung des Testens im Entwicklungsalltag zu verdeutlichen. Da Unit-Tests einen wichtigen Bestandteil der Arbeit repräsentieren, werden Grundlagen zu diesen Tests geschaffen und der Bezug zur Programmiersprache Java hergestellt.

\section{Software Development Lifecycle}
Der SDLC ist ein Grundlagenzyklus, welcher innerhalb eines Softwareprojektes festgelegt und während der Bearbeitung des Projektes befolgt wird. \cite*{SoftwareDevelopmentLife} Dieser dient zur Verbesserung von Qualität und Entwicklung.\input{Assets/LaTeX/SDLC.tex}Ein typischer SDLC besteht aus 6 verschiedenen Phasen. [Abb. \ref{fig:sdlc}]\\\\
\textbf{Planung}\\[0.2cm]
Eine Planung erfolgt in der Regel durch eine neue Innovation, welche von einem Endnutzer oder Sponsor ausgeht. \cite*{tikySoftwareDevelopmentLife} Hierbei werden Projektumfang, Ziele sowie Anforderungen definiert. Dabei steht vor allem der Projektplan im Fokus, welcher Zeitrahmen und Ressourcen definiert. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Entwurf}\\[0.2cm]
In der Entwurfsphase wird festgelegt, wie benötigte Softwarekomponenten zusammenwirken und eine Produktarchitektur definiert. Es kommt zum Entwerfen des \textit{User Interfaces} (engl. Benutzeroberfläche) mit Blick auf die \textit{User Experience} (engl. Benutzererfahrung). \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Entwicklung und Implementierung}\\[0.2cm]
Nach Abschluss aller vorher nötigen Phasen, beginnen Programmierer mit der Entwicklung und Implementation der Software bei Einhaltung aller festgelegten Anforderungen und entsprechend dem Architekturentwurf. \cite*{tikySoftwareDevelopmentLife} \\\\
\textbf{Testen}\\[0.2cm]
Das Testen der Software ist eine wichtige und essenzielle Phase, die während sowie nach der Entwicklung stattfindet. Hierbei kommt es zur Durchführung verschiedener Teststufen, einschließlich Unit-Tests, Integration Tests, System Tests und Benutzerakzeptanz Tests. Es werden Fehler identifiziert und behoben sowie Anforderungen aus vorherigen Phasen überprüft. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
\textbf{Deployment und Wartung}\\[0.2cm]
Zum \textit{Deployment} gehört die Bereitstellung der Software in einer Staging-Umgebung. Diese wird vorausgesetzt, um abschließende Tests durchzuführen und die Software zu validieren. Es werden Server und Datenbanken konfiguriert, sodass eine Produktionsumgebung vorbereitet wird. Nach erfolgreichen \textit{Rollout} (engl. ausrollen) der Software steht die Überwachung und Wartung der Produktionsumgebung im Fokus. \cite*{hossainSoftwareDevelopmentLife2023}\\\\
Insbesondere die Phase des Testens sollte als eine \textit{Sub stage} (engl. Unterstufe) jeder Phasen betrachtet werden, da ein Testen von Anforderungen, Implementation, Code sowie Deployment dauerhaft notwendig ist. Aufgrund dessen ist die Testphase besonders komplex und aufwändig, weshalb der Fokus ständig aufrechterhalten werden muss. \cite*{tikySoftwareDevelopmentLife}

\section{Large Language Models}
\subsection{Begriff und Einführung}
Hinter dem simplen Begriff \textit{Large Language Model} (engl. Großes Sprachmodell) steckt eine komplizierte Technologie mit großer Herkunftsgeschichte, welche eine Definition mehrerer Begrifflichkeiten im Feld der Künstlichen Intelligenz (kurz KI) voraussetzt. Dadurch kann ein Grundverständnis geschaffen werden, um den Begriff LLM besser einordnen zu können. \cite{seemannKuenstlicheIntelligenzLarge} \\\\ Ein Hauptbestandteil der KI spielen \textbf{Künstliche Neuronale Netze} (kurz \acs{KNN}). 
Die Inspiration dieser Netze entstammt dem Neuronalen Netzwerk im Gehirn eines Menschen. \cite*{seemannKuenstlicheIntelligenzLarge} Das Ziel dabei ist, natürliche Organismen nachzubauen, und somit lernfähige Programme zu erzeugen. 
\input{Assets/LaTeX/KNN}Der Aufbau eines solchen Netzes folgt dem Vorbild der Struktur des menschlichen Gehirns und besteht somit aus verbundenen künstlichen Nervenzellen (Neuronen), welche durch eine Eingabeschicht passende Ausgaben erzeugen. \cite*{morrKuenstlicheNeuronaleNetze} Unterschieden wird zwischen 3 verschiedenen Arten von Neuronen. Somit besteht ein Netz grundlegend aus Input-, Hidden- und Outputneuronen, zwischen denen eine gewichtete Verbindung herrschen kann. \cite*{NeuronaleNetzeEinfuhrung} [Abb. \ref{fig:knn}] Durch solch eine Struktur können KNN durch maschinelles Lernen mit großen Datenmengen trainiert werden und dadurch Fähigkeiten wie das Generieren von Texten erlernen. \cite*{seemannKuenstlicheIntelligenzLarge}\\\\ Ein weiteres Teilgebiet in der KI ist das \textbf{Natural Language Processing} (engl. natürliche Sprachverarbeitung, kurz NLP). Hierbei liegt der Fokus auf der Interaktion zwischen Computern und menschlicher Sprache, in dem Algorithmen und Modelle ermöglichen, dass Computer die menschliche Sprache verstehen, interpretieren und auf sinnvolle Weise erzeugen können. \cite*{muruganNaturalLanguageProcessing2024} \\\\ Dies schafft die Grundlage eines \textbf{Large Language Models}, denn diese Modelle basieren auf KNN und sind spezialisiert auf die natürliche Sprachverarbeitung. \cite*{seemannKuenstlicheIntelligenzLarge} Sie sind ausgelegt, um Beziehungen zwischen einzelnen Daten zu lernen und Sequenzen vorherzusagen. Insbesondere deswegen sind sie für die Generierung von Text in natürlicher Sprache geeignet, da sie die Fähigkeit besitzen Sprache aus großen Datensätzen von geschriebenen Texten im Internet sowie digitalisierten Büchern zu erlernen, um sich in neuartigen Situationen angepasst verhalten zu können. \cite*{okerlundWhatChatterbox2022} \cite*{authorityofthehouseoflordsLargeLanguageModels} \\Ein \textit{pre-training} (engl. Vortraining) sowie \textit{fine-tuning} (engl. Feinabstimmung) ist in vielen Fällen notwendig, um die Genauigkeit des Modells auf bestimmte Bereiche zu verbessern. \cite{HowLLMsWork} Um eine solche Genauigkeit zu gewährleisten, arbeiten Sprachmodelle mit dem Prinzip der \textit{Tokenisierung}. Im Rahmen dieses Vorverarbeitungsschritts erfolgt eine Zerlegung des Textes in Token, wobei diese aus Zeichen, Teilwörtern oder Symbolen bestehen können. \cite*{naveedComprehensiveOverviewLarge2024} \\ Grundlegend nehmen LLM's einen Prompt als Eingabe entgegen und generieren abhängig von gesetzten Parametern des Modells, sowie bisher erlernten Informationen, eine Ausgabe. \input{Assets/LaTeX/Word_process.tex}Das vorhergesagte Wort ist das Ergebnis einer Wahrscheinlichkeitsverteilung des Modells und wird so lange mit der Sequenz verknüpft, bis sie endgültig ist. [Abb. \ref{fig:word}] Dabei können unterschiedliche Ergebnisse erzielt werden, welche abhängig von der \textit{sampling strategy} (engl. Stichprobenstrategie) sind. \cite*{cholletDeepLearningPython2018} Ein Beispiel wäre \textit{sampling with temperature} (engl. Stichprobeverfahren mit Temperatur-Parameter), bei dem die Wahl der Temperatur entscheidungsgebend ist. Wählt man bspw. einen Wert nahe 0, so wird das Sprachmodell in den meisten Fällen das Ergebnis mit der höchsten Wahrscheinlichkeit nutzen. Umso höher der Wert, desto eher wird das Ergebnis mit einer niedrigeren Wahrscheinlichkeit genutzt. \cite*{WeightsBiases} Dies bedeutet, um auf die Qualität der Ergebnisse Einfluss zu nehmen, gibt es die Möglichkeit manuelle Parameter zu setzen. Darunter zählen z.B. \textit{max tokens} zum Festlegen der maximalen Anzahl an Tokens, die generiert werden dürfen oder auch, wie oben erwähnt, die \textit{temperature}, welche die Zufälligkeit von 0.0 (deterministisch) - 2.0 (extrem zufällig) regelt.\\ Im Allgemeinen finden LLM's eine große Spannbreite an Anwendung, wie bspw. Textgenerierung, Vervollständigung und Zusammenfassung. \cite*{dhadukHowLargeLanguage2023}

\subsection{Generative Pre-trained Transformer}
Unter allen Sprachmodellen, wie bspw. FLAN oder PaLM, sticht die Serie der \textit{Generative Pre-trained Transformer} (engl. vortrainierter generativer Transformator, kurz GPT) durch ihre besonders hohe Performance hervor. \cite*{yeComprehensiveCapabilityAnalysis2023} Die in GPT verwendete Transformer-Architektur ist ein bedeutender Fortschritt gegenüber früheren \acs{NLP}-Ansätzen. Durch ihren Mechanismus der Selbstbeobachtung ermöglicht sie es dem Modell, bei der Wortgenerierung, den Kontext des gesamten Satzes zu berücksichtigen. \cite*{yenduriGenerativePretrainedTransformer2023} \\ OpenAI's Sprachmodelle sind Teil dieser Serie und haben mit GPT-3.5 Turbo, GPT-4 sowie GPT-4o drei der populärsten LLM's auf den Markt gebracht. \cite*{yaoSurveyLargeLanguage2024}\\
\textbf{GPT-4o} ist das am weitesten fortgeschrittene Modell. Neben seiner Multimodalität besitzt es die gleiche Intelligenz wie GPT-4, ist jedoch viel effizienter. Es erzeugt Text doppelt so schnell, ist 50\% billiger und ebenso das aktuellste Modell von OpenAI. Somit rückt GPT-4o in den Fokus dieser Arbeit. \cite*{OpenAIPlatform} Tabelle \ref{fig:gpt} zeigt alle relevanten Daten und Fakten des LLM.
\input{Assets/LaTeX/GPT-Table.tex}


\section{Prompt Engineering}
Das \textit{Prompt-Engineering} spiegelt eine Technik zur Verbesserung der Fähigkeiten von LLM's wieder. Es werden sogenannte \textit{Prompts} genutzt, welche eine strategische und spezifische Anweisung repräsentieren, um die Modellausgabe zu steuern. Somit wird ein Mechanismus bereitgestellt, der ein \textit{fine tuning} des \textit{Outputs} ermöglicht und das Modell anpassungsfähiger macht. \cite*{sahooSystematicSurveyPrompt2024} \input{Assets/LaTeX/Prompt-Eng.tex}

\subsection{Prompt Elemente}
Der Aufbau eines Promptes besteht aus folgenden Elementen [Abb. \ref{fig:prompt-eng}]:
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item \textbf{Anweisung} - eine Anweisung, die das Modell ausführen soll
    \item \textbf{Kontext} - Zusatzkontext, der das Modell zu besseren Antworten lenken kann
    \item \textbf{Eingabedaten} - die Eingabe, für die eine Antwort gefunden werden soll
    \item \textbf{Ausgabeindikator} - das Format der Ausgabe
\end{itemize}
Es sind nicht alle Elemente nötig um einen Prompt zu kreieren, bspw. benötigt es keinen Kontext, wenn die Eingabedaten für genug Klarheit schaffen. \cite*{ElementsPromptNextra2024}

\subsection{Prompt Techniken}
Wenn Aufgaben an Größe und Komplexität zunehmen, erzielt ein einfacher \textit{Prompt} in den meisten Fällen nicht das gewünschte Ergebnis. Zur Behebung dieses Problems gibt es verschiedene \textit{Prompt Techniken}, die zu mehr Zuverlässigkeit und Leistung von LLM's führen. Die gewöhnlichsten und zugleich effektivsten Techniken sind der Zero-Shot Prompt sowie Few-Shot Prompt. \cite*{PromptEngineeringGuide}\\\\
\textbf{Zero-Shot Prompt}\\[0.2cm]
Wie in vorherigen Kapiteln beschrieben, werden Sprachmodelle wie GPT-3.5-turbo oder GPT-4-turbo anhand großer Datenmengen trainiert und sind auf die Befolgung von Anweisungen abgestimmt. Dadurch sind sie in der Lage Aufgaben anhand eines \textit{Prompts} zu lösen, welcher keine Beispiele oder Demonstrationen enthält. Dies bedeutet, dass der \textit{Zero-Shot Prompt} das Modell anweist, ohne zusätzliche Beispiele zur Steuerung anzugeben. Ein Beispiel ist in Abbildung \ref{fig:prompt-zero} zu sehen.\input{Assets/LaTeX/Prompt-Zero.tex}Bei komplexeren Anforderungen kann es dazu kommen, dass ein \textit{Zero-Shot Prompt} nicht funktionert bzw. nicht zum erwarteten Ergebnis führt. In diesem Fall empfiehlt es sich, im \textit{Prompt} zusätzlichen Kontext in Form von Demonstrationen oder Beispielen hinzuzufügen. \cite{PromptEngineeringGuide}\\\\
\textbf{Few-Shot Prompt}\\[0.2cm]
\textit{Few-Shot Prompting} wird als Technik verwendet, um kontextbezogenes Lernen durch Demonstrationen und Beispiele, innerhalb des \textit{Prompts}, zu ermöglichen und dadurch das Modell zu einer besseren Leistung zu führen. Hierbei eröffnet sich die Möglichkeit, die Lösung der Aufgabe in eine gewünschte bzw. bestimmte Richtung zu steuern. Abbildung \ref{fig:prompt-one} zeigt ein Beispiel für einen \textit{Few-Shot Prompt}. \input{Assets/LaTeX/Prompt-One.tex}\\
Zusammenfassend lässt sich sagen, dass \textit{Prompt Engineering} Anweisungen nutzt, die ein \textit{fine tuning} der Leistung von vortrainierten LLM's basierend auf spezifischen \textit{Prompts} ermöglicht, und somit die Vielseitigkeit und Anwendung in unterschiedlichen Bereichen verbessert. \cite*{sahooSystematicSurveyPrompt2024}

\section{Unit Testing}
Ein Hauptproblem im Softwareentwicklungszyklus ist häufig die fehlende Zeit, um ein komplettes Projekt vor dem \textit{Release} manuell zu testen. \cite*{SoftwareQualityLab} Eine Lösungsvariante für diese Art von Problemstellungen bieten \textit{Unit-Tests}. 

\subsection{Grundlagen}
\textit{Unit-Tests} werden von Entwicklern geschrieben, um eine bestimmte Funktionalität in einem Ausschnitt des Codes zu testen. Somit wird es ermöglicht problematische Bereiche im Code offenzulegen und Fehler zu reduzieren. \cite*{tosunEffectivenessUnitTests2018} Die Tests werden in der gleichen Programmiersprache geschrieben wie der Produktionscode und bilden die Basis der Testing Pyramide (Abb. \ref{fig:pyramid}). \input{Assets/LaTeX/Pyramid.tex}Idealerweise werden sie häufig und automatisiert durchgeführt, um sicherzustellen, dass neue Änderungen die bisherig bestehende Funktionalität nicht zerstören. \cite*{TestingPyramidStrategic2024}\\
Der Prozess des \textit{Unit-Testing} besteht aus grundlegend 4 Phasen. Die \textbf{Planung} dient zur Überlegung und Analyse, um entscheiden zu können welche Codeeinheiten getestet werden müssen. Dabei ist es essenziell alle relevanten Funktionen der Einheiten abzudecken. \cite*{bakharevUnitTestingDefinition2023} Die zweite Phase dient zum \textbf{Schreiben der Testfälle}, in der ein Skript angefertigt und Code für den \textit{Unit-Test} geschrieben wird. \cite*{bakharevUnitTestingDefinition2023} Beim \textbf{Ausführen} der Tests wird erkennbar, wie sich der Programmcode bei jedem Testfall verhält und eine \textbf{Analyse} ermöglicht, in der Fehler und Probleme im Code erkannt und behoben werden können.\cite*{bakharevUnitTestingDefinition2023}\\\\
Der \textit{Unit-Test} selbst, basiert meist auf einem 3A-Pattern: Arrange, Act, Assert. \\\textbf{Arrange} initialisiert die benötigen Objekte und setzt, wenn verlangt, \textit{mocks} (engl. Attrappen) um einen Platzhalter oder eine Attrappe für ein Objekt bereitzustellen.\\ Der \textbf{Act}-Abschnitt ruft die zu testende Funktion mit den im vorherigen Schritt initialisierten Objekten auf und mithilfe des \textbf{Assert}-Abschnitts wird überprüft, dass sich die Funktion wie erwartet verhält. \cite*{ikechiHowStructureUnit2021} [Quellcode \ref{lst:java-method-example} und \ref{lst:java-unit-example}]\\
\lstinputlisting[caption=Java-Methode Beispiel,captionpos=b,label={lst:java-method-example},language=Java]{Assets/Code/Example-Func.java}
\vspace{2cm}
\lstinputlisting[caption=Unit-Test Beispiel,captionpos=b,label={lst:java-unit-example},language=Java]{Assets/Code/Example-Unit.java} Somit sind \textit{Unit-Tests} eine wichtige Maßnahme zur Qualitätssicherung und ermöglichen das frühzeitige Finden und Beheben von Fehlern.

\subsection{Java mit JUnit5}
JUnit ist ein Unit-Testing-Framework, welches zum Testen in Java Anwendungen verwendet wird. Die aktuellste Version des \textit{Frameworks} ist \textit{JUnit5}, das eine moderne Grundlage für Entwickler-seitige Tests bietet. \cite*{JUnit} JUnit5 unterstützt die automatisierte Durchführung von Tests und trägt somit zur Effizienzsteigerung aus Sicht des Entwicklungsprozesses bei. \cite*{venkatesanJunitFrameworkUnit} Es werden Werkzeuge und nützliche \textit{Features} zur Verfügung gestellt, um das Implementieren von Testfällen zuverlässiger und schneller zu gestalten. Einige der wichtigsten und am häufigsten verwendeten \textit{Features} sind dabei Annotationen, Testklassen und Assertions (engl. Behauptungen). \cite*{venkatesanJunitFrameworkUnit}\\
Annotationen sind \textit{Tags}, welche zunächst dem Java-Code hinzugefügt werden, um die Lesbarkeit und Struktur des Programmcodes zu verbessern. Ebenso definieren sie die Reihenfolge von Codeteilen innerhalb des Tests. Einige Beispielannotationen sind: \cite*{JUnitUserGuide}
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item \textbf{@Test} - zeigt, dass eine Methode eine Testmethode ist
    \item \textbf{@BeforeEach} - legt fest, dass die Methode vor jedem @Test ausgeführt wird
    \item \textbf{@AfterAll} - legt fest, dass die Methode nach allen @Test ausgeführt wird
\end{itemize}
Testklassen können mehrere Testmethoden beinhalten, was ein Ausführen von mehr als einer Testmethode vereinfacht und für strukturelle Vorteile wie die Lesbarkeit innerhalb des Projektes sorgt. Quellcode \ref{lst:java-class-example} zeigt ein Beispiel:\\
\lstinputlisting[caption=Testklasse Beispiel,captionpos=b,label={lst:java-class-example},language=Java]{Assets/Code/Example-Class.java}
Assertions finden in der Praxis häufig Anwendung und stellen weitere nützliche Methoden zum Schreiben von Testfällen dar. Einige der wichtigsten Methoden demonstriert Quellcode \ref{lst:assertions-example}.\\
\lstinputlisting[caption=Assertion-Methoden,captionpos=b,label={lst:assertions-example},language=Java]{Assets/Code/Assertions.java}
Die einfachste Möglichkeit JUnit5 einzurichten, ist die Verwendung von \textit{Apache Maven}. Hierbei handelt es sich um ein weit verbreitetes Build-Automation-Tool für Java-Anwendungen. Zur Einrichtung reicht ein einfaches Hinzufügen der JUnit-Dependency zum \textit{Project Object Model} (engl. kurz POM) in der pom.xml. \cite*{baeldungGuideJUnitBaeldung2016}

\section{SonarQube}
In den vorherigen Kapiteln wurde deutlich, dass eine Testabdeckung des Projekts mit \textit{Unit-Tests} ein entscheidender Faktor für die Qualität des Programmcodes ist. Allerdings stellt sich die Frage, anhand welcher Kriterien sich die Qualität der Tests beurteilen lässt und wie sichergestellt werden kann, dass der Testcode die Anwendung tatsächlich umfassenden abdeckt. Ein solches selbst verwaltendes und automatisches Code-Review-Tool stellt SonarQube bereit. Dabei lässt es sich in den bestehenden Arbeitsablauf integrieren und erkennt Probleme im Programmcode sowie im Testcode. \cite*{SonarQube10} Des Weiteren werden Metriken bereitgestellt, die eine Bewertung des eigenen Testcodes ermöglichen und dadurch eine Einschätzung der Abdeckung der Anwendung zulassen.\\\\
Eine erste entscheidende Metrik ist die \textbf{Code Coverage} (engl. Codeabdeckung) der Tests. Anhand dieser \textit{Coverage} lässt sich die Frage beantworten, wie viel des Quellcodes durch \textit{Unit-Tests} abgedeckt wurde. Durch folgende Gleichung lässt sich eine Abdeckung in \% berechnen \cite*{MetricDefinition}:\\\\
$Coverage = \frac{(CT + CF + LC)}{2 \cdot B + EL}$
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item CT - Bedingungen, die mindestens einmal als \textit{true} ausgewertet wurden
    \item CF - Bedingungen, die mindestens einmal als \textit{false} ausgewertet wurden
    \item LC - abgedeckte Quellcodezeilen
    \item B - Gesamtzahl an Bedingungen
    \item EL - Gesamtzahl an ausführbaren Quellcodezeilen
\end{itemize}
Im Allgemeinen wird eine Codeabdeckung durch \textit{Unit-Tests} von 60\% - 90\% angestrebt. \cite*{WhatReasonableCode}\\
Eine weitere Metrik, die zur \textit{Code Coverage} beiträgt, ist die \textbf{Condtion Coverage} oder auch \textbf{Branch Coverage} (engl. Bedingungs-/Zweigabdeckung). Hierbei wird zu jeder Quellcodezeile, die eine Verzweigung enthälgt, überprüft, ob der Ausdruck einmal für \textit{true} und einmal für \textit{false} ausgewertet wurde. Durch folgende Gleichung wird ein Ergebnis in \% ausgegeben \cite*{MetricDefinition}:\\\\
$Condition \,\, Coverage = \frac{(CT + CF)}{2 \cdot B}$
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item CT - Bedingungen, die mindestens einmal als \textit{true} ausgewertet wurden
    \item CF - Bedingungen, die mindestens einmal als \textit{false} ausgewertet wurden
    \item B - Gesamtzahl an Bedingungen
\end{itemize}
Die \textbf{Line Coverage} stellt ebenso ein Bestandteil der \textit{Code Coverage} dar und gibt an, wie viel \% der Quellcodezeilen durch die Tests ausgeführt werden \cite*{MetricDefinition}:\\\\
$Line \,\, Coverage = \frac{LC}{EL}$
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item LC - abgedeckte Quellcodezeilen
    \item EL - Gesamtzahl an ausführbaren Quellcodezeilen
\end{itemize}
Wichtig ist, die Erfolgsquote der Tests auszuwerten. Dazu wird die Gesamtzahl der \textit{Unit-Tests} benötigt, um eine Quote angeben zu können. Man unterscheidet für die Berechnung zwischen \textit{Errors} und \textit{Failures}. Bei einem \textit{Unit-Test Error} handelt es sich um einen unerwarteten Fehler wie bspw. einem \textit{FileNotFound} Error wohingegen es sich bei einem \textit{Unit-Test Failure} um ein Fehlschlagen des Testfalls (bspw. bei einer falschen \textit{Assertion}) handelt. Die Quote selber, lässt sich wie folgt berechnen \cite*{MetricDefinition}:\\\\
$Success = \frac{(Tests - (Errors + Failures))}{Tests \cdot 100}$\\\\
Durch die oben genannten Metriken, lässt sich die Qualität der \textit{Unit-Tests} aus verschiedenen Blickwinkeln evaluieren.\\\\
Eine ausführliche Analyse benötigt eine Auswertung an sowohl simplen bis hin zu komplexen Projekten. SonarQube stellt mit der \textbf{Complexity} oder auch \textbf{Cyclomatic Complexity} (engl. zyklomatische Komplexität) eine Metrik bereit, welche für eine spezifische Sprache die Komplexität berechnet. Diese wird durch die Anzahl \textit{function splits} (engl. Funktionsaufteilungen) definiert. Für die Programmiersprache Java erhöht sich die Komplexität somit bei jedem \colorbox{gray!20}{if}, \colorbox{gray!20}{for}, \colorbox{gray!20}{while}, \colorbox{gray!20}{case}, \colorbox{gray!20}{\&\&}, \colorbox{gray!20}{||}, \colorbox{gray!20}{?} und \colorbox{gray!20}{->}. \cite*{MetricDefinition}
