\chapter{Durchführung und Analyse}
Im fünften Kapitel erfolgt die Durchführung der Generierung von Tests mit \textit{Unitcraft} anhand verschiedener Projekte. Im Anschluss erfolgt eine Analyse der Ergebnisse, welche auf Basis zuvor definierter Metriken und eines klar definierten Bewertungsverfahren durchgeführt wird. Es findet eine Codeanalyse statt, welche auf Unterschiede in den Tests eingeht und anhand der Ergebnisse Schlüsse zieht. Abschließend werden die erzeugten Metriken mit den Metriken der menschlichen Tester verglichen.

\section{Durchführung}
Die Durchführung findet an einer Auswahl von Projekten mit unterschiedlichen Komplexitätsgraden und Größen statt. Die Projekte entstammen dem Unternehmen \textit{SENEC GmbH} und werden mit `Projekt x` (x für eine Zahl) benannt. Projekt 1 besitzt eine Komplexität von 33 und ist mit einer Anzahl an Codezeilen von 318 das kleinste Projekt. Es folgt Projekt 2 mit einer Komplexität von 102 und einer Codezeilenanzahl von 730. Das größte und komplexeste Projekt ist Projekt 3. Die Komplexität beträgt 124 und die Zeilen an Quellcode 1231. [Tabelle \ref{fig:project}]
\input{Assets/LaTeX/Project-Table}Für jedes Projekt werden 6 Testdurchläufe durchgeführt. Diese entstehen durch die Kombination von Prompt-Technik und dem Temperaturparameter. Somit wird der \textit{Zero Shot Prompt} sowie der \textit{One Shot Prompt} mit der Temperatur von 0, 0.25 und 0.5 für die Generierung verwendet. Insgesamt erzielen wir 18 Testdurchläufe, welche dafür sorgen, dass eine aussagekräftige Anzahl an Tests generiert wird.

\section{Evaluation der Tests}
Zur Bewertung und Analyse der Testergebnisse werden folgende Metriken benutzt: 
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item Line Coverage
    \item Branch Coverage
    \item Overall Coverage (kurz Coverage)
    \item Erfolgsquote mit Anzahl an Unit Tests
\end{itemize}
Jede Metrik wird projektweise evaluiert und bildet Durchschnittswerte aus allen Projekten. Anhand dieser Durchschnittswerte werden die Ergebnisse in Kategorien eingegliedert. Eine Einteilung der Coverage nach bestimmten Prozentwerten ist nicht sinnvoll, da es keinen Wert gibt, der auf alle Projekte zutrifft. Aufgrunddessen wird der Prozentbereich von 60\% - 90\% definiert und unterschieden, ob die Coverage in diesem angestrebten Bereich liegt oder nicht. \cite{WhatReasonableCode}

\subsection{Line Coverage}
Tabelle \ref{fig:line-1} zeigt die Ergebnisse der \textit{Line Coverage} für Projekt 1. Den besten Wert mit 72.4\% erzielt der \textit{Zero Shot} in Kombination mit einer Temperatur von 0.25. Der \textit{Zero Shot} mit einer Temperatur von 0.5 schneidet mit 63.5\% am schlechtesten ab.
\input{Assets/LaTeX/Line-Cov-1.tex}
Die Ergebnisse der Line Coverage für Projekt 2 sind in Tabelle \ref{fig:line-2} dargestellt. Mit 84.9 \% erreicht der \textit{One Shot} mit einer Temperatur von 0.25 den höchsten Wert. Der \textit{Zero Shot} mit Temperatur 0 schneidet mit über 20\% schlechter ab.
\input{Assets/LaTeX/Line-Cov-2.tex}
Tabelle \ref{fig:line-3} stellt die Ergebnisse der Line Coverage für Projekt 3 dar. Mit 62.7\% erzielt der One Shot mit einer Temperatur von 0 den besten Wert. Der schlechteste Wert ist 47.3\% und entsteht in der Kombination von One Shot und einer Temperatur von 0.5.
\input{Assets/LaTeX/Line-Cov-3.tex}
In Tabelle \ref{fig:line-avg} ist der Durchschnitt aller Ergebnisse dargestellt. Alle Werte treffen den angestrebten Bereich von 60\% - 90\%. Den besten Wert erzielt der One Shot mit einer Temperatur von 0.25. Am schlechtesten schneidet der Zero Shot mit einer Temperatur von 0.5 ab. Es ist klar erkennbar, dass eine hohe Temperatur wie 0.5, im Vergleich zum Rest deutlich schlechter abschneidet. Auch der One Shot Prompt erreicht im Vergleich zum Zero Shot Prompt die besseren Ergebnisse.
\input{Assets/LaTeX/Line-Cov-avg.tex}

\subsection{Branch Coverage}
Wie in Tabelle \ref{fig:branch-1} dargestellt, beträgt die Branch Coverage für Projekt 1 in allen Kombinationen 50.0\%. Somit schneidet jeder Prompt gleich ab.
\input{Assets/LaTeX/Branch-Cov-1.tex}
In Tabelle \ref{fig:branch-2} erreicht der One Shot Prompt mit einer Temperatur von 0.25 erneut das beste Ergebnis der Branch Coverage für Projekt 2.
\input{Assets/LaTeX/Branch-Cov-2.tex}
Für Projekt 3 fallen die Werte erheblich schlechter aus, als im Vergleich zum Rest. Der One Shot Prompt erzielt mit 32.0\% und einer Temperatur von 0 das beste Ergebnis. Die Temperatur von 0.5 schneidet am schlechtesten ab.
\input{Assets/LaTeX/Branch-Cov-3.tex}
Tabelle \ref{fig:branch-avg} zeigt die Durchschnittswerte der Branch Coverage. Es ist eine klare Verschlechterung im Vergleich zur Line Coverage zu erkennen. Im Schnitt erreicht der One Shot Prompt mit der Temperatur 0.25 erneut den besten Wert. Auch der Zero Shot Prompt schneidet in Kombination mit einer Temperatur von 0.5 erneut am schlechtesten ab. Des Weiteren wird deutlich, dass die Temperatur von 0.5 die schlechtesten Werte erzielt. Der Vergleich der Prompts ist ausgeglichener als zuvor, jedoch liegen alle Ergebnisse unter dem vorher festgelegten Idealbereich. 
\input{Assets/LaTeX/Branch-Cov-avg.tex}

\subsection{Overall Coverage}
Tabelle \ref{fig:o-1} stellt die Coverage Ergebnisse für Projekt 1 dar. 72.4\% ist der Höchstwert, erreicht durch die Kombination aus Zero Shot und der Temperatur 0.25. Die Temperatur 0.5 schneidet erneut am schlechtesten ab. Betrachtet man die Werte im Gesamten, erreicht das Sprachmodell ein zufriedenstellendes Ergebnis, da kein Wert unter 60\% fällt.
\input{Assets/LaTeX/O-Cov-1.tex}
Mit einer Overall Coverage von 81.0\% erzielt die Kombination aus One Shot Prompt und einer Temperatur von 0.25 den besten Wert für Projekt 2. [Tabelle \ref{fig:o-2}] Der schlechteste Wert beträgt 59.0\% und liegt somit unter der 60\% Grenze. Der Rest erreicht ein zufriedenstellendes Ergebnis.
\input{Assets/LaTeX/O-Cov-2.tex}
Eine klare Verschlechterung erkennt man im komplexesten und größten Projekt, dem Projekt 3. Tabelle \ref{fig:o-3} stellt die Ergebnisse tabellarisch dar. Es wird direkt erkennbar, dass keine Kombination die 60\% erreicht, und der höchste Wert bei einer Coverage von 53.5\% liegt. Hier wird deutlich, dass sich die Ergebnisse des Sprachmodells bei zunehmender Komplexität verschlechtern.
\input{Assets/LaTeX/O-Cov-3.tex}
Tabelle \ref{fig:o-avg} zeigt die Durchschnittswerte und verdeutlicht ein zufriedenstellendes Ergebnis, da 3 von 6 Ergebnissen den angestrebten Bereich erreichen. Wie auch in den vorherigen Metriken, erzielt die Kombination aus One Shot und der Temperatur 0.25 den besten Wert. Die Temperatur von 0.5 erlangt die schlechtesten Ergebnisse und der One Shot Prompt ist dem Zero Shot prompt sichtbar überlegen.
\input{Assets/LaTeX/O-Cov-avg.tex}

\subsection{Erfolgsquote}
Da die Erfolgsquote in allen Projekten ähnliche Werte aufweist und somit keine klare Unterscheidung aufgrund von Komplexität und Größe des Projektes getroffen werden kann, wird zur Evaluation dieser Quote der durchschnittliche Erfolg, auf Basis aller Projekte verwendet.\\
In Tabelle \ref{fig:succ} ist erkennbar, dass die Erfolgsquote zwischen 42.7\% (Zero Shot mit Temperatur von 0.5) und 46.0\% (One Shot mit Temperatur von 0.25) liegt. \input{Assets/LaTeX/Success.tex}Die in Tabelle \ref{fig:unit} dargestellten durchschnittlich erzeugten Unit Tests dienen der Berechnung der tatsächlich erfolgreichen Tests.\input{Assets/LaTeX/Unit.tex}Dabei multiplizieren wir die Anzahl der Unit Tests mit der Erfolgsquote. Das Ergebnis in Tabelle \ref{fig:succ-unit} stellt die tatsächlich erfolgreichen Unit Tests dar, die nicht aufgrund eines Erros während der Ausführung, fehlschlugen. \input{Assets/LaTeX/Succ-Unit.tex} Es wird klar, dass weniger als die Häflte der generierten Unit Tests erfolgreich ausgeführt werden. Dadurch entsteht ein großer Anteil an irrelevantem Code im Projekt, was die Quellcode- sowie Projektqualität erheblich verschlechtert. Aus einem anderen Blickwinkel wird ebenso Potenzial für die Testabdeckung verschwendet, wenn ein Unit Test aufgrund eines simplen Errors wie bspw. einer \textit{NullPointerException} fehlerhaft ist.

\section{Testcodeanalyse}
Errors während der ausführung beeinflussen Testunterschiede kaum --> Errors die zu Kompilierfehlern führen 
Zum Aufzeigen von Unterschieden --> beste und schlechteste Kombination mit code bsp
meiste unterschiede: - imports, packages, private access
Ergebnisse von 0 und 0.25 deutlich besser als 0.5
-> randomness könnte dinge vergessen
Ergebnisse von One besser als Zero
-> durch beispielcontext eher an imports etc gedacht\\\\
Tabelle \ref{fig:succ} verdeutlicht, dass sich die Erfolgsquote aller Kombinationen aus Prompt-Technik und Temperatur kaum unterscheiden. Daraus lässt sich erschließen, dass die Unterschiede in den Coverage-Ergebnissen nicht aus dem Erfolg der generierten Tests ableitbar sind. Eine genauerer Blick auf den Testcode ist notwendig, um Tabelle \ref{fig:o-avg} zu analysieren. Dazu wird der generierte Testcode mit der schlechtesten Overall-Coverage (Zero Shot mit Temperatur von 0.5) sowie der besten Overall-Coverage (One Shot mit Temperatur von 0.25) verglichen, um Unterschiede nachvollziehen zu können. Es wird deutlich, dass fehlende Packages, Imports und \textit{Private-Access-Errors} die stärksten Hürden darstellen, und einen Kompilierfehler erzwingen, sodass der generierte Test vom Tools gelöscht wird. \\\\
Quellcode \ref{lst:missing-package} zeigt ein solches Vergessen des Packages, welches ein \textit{Missing package statement: 'com.senec.emsrelay'} verursacht und somit ein erfolgreiches Kompilieren des Tests verhindert.\\
\lstinputlisting[caption=Fehlendes Package in Testklasse,captionpos=b,label={lst:missing-package},language=Java]{Assets/Code/Missing-Package.java}
In Quellcode \ref{lst:missing-import} ist ein fehlender Import dargestellt, welcher ebenso einen Kompilierfehler verursacht, da die Methode \textit{assertEquals} somit nicht bereitgestellt und das Löschen der gesamten Testklasse erzwungen wird.\\
\lstinputlisting[caption=Fehlender Import in Testklasse,captionpos=b,label={lst:missing-import},language=Java]{Assets/Code/Missing-Import.java}
Obwohl alle Prompts so designt sind, dass die Verwendung von Reflection für den Zugriff auf Private Methoden oder Attribute empfohlen wird, ist vermehrt sichtbar, dass bei der Wahl der Temperatur von 0.5 diese Anweisung ignoriert und somit durch einen \textit{Privat-Acces-Error} die Ausführung des Tests verhindert wird.\\
Betrachtet man den Einfluss der Temperatur in Abbildung \ref{fig:temp} ist eine Verschlechterung beim Anstieg über 0.25 erkennbar. \input{Assets/LaTeX/Temperature} Betrachtet man die oben genannten Unterschiede im Testcode im Zusammenhang mit Abbildung \ref{fig:temp} kann man schlussfolgern, dass durch eine höhere Temperatur und somit einer höheren Kreativität der Fokus wichtiger Dinge verloren geht. Eine deterministischer Wert sorgt für bessere Ergebnisse in der Coverage. Hierbei schneidet jedoch die Temperatur 0.25 besser ab als die Temperatur 0.5, was vermuten lässt, dass durch einen kleinen Anteil an Kreativität, unerwartete Hürden wie bspw. der Zugriff auf private Methoden oder zusätzlich notwendige Imports besser gelöst werden.\\\\
Doch nicht nur die Temperatur nimmt Einfluss auf die Lösung oben genannter Fehlerursachen. Abbildung \ref{fig:prompt} zeigt, dass der One Shot Prompt bessere Ergebnisse erzielt, als der Zero Shot Prompt \input{Assets/LaTeX/Prompt}Daraus lässt sich ableiten, dass durch das Hinzufügen eines Beispielkontextes innerhalb eines One Shot Prompts die oben genannten Hürden besser gelöst werden. Dies ist auf den zusätzlichen Kontext zurückführen, da dieser durch beispielhaft korrektes Einbinden von Imports und Packages eine Vorlage bietet.\\\\
Schlussfolgernd lässt sich sagen, dass durch Einfluss beider Faktoren und den eben gezeigten Vorteilen der jeweiligen Prompt Technik sowie Temperatur, der One Shot Prompt mit einer Temperatur von 0.25 die besten Voraussetzungen für das Lösen auftretender Fehler bietet und Eigenschaften wie die Kreativität und zusätzlicher Kontext das Ergebnis maßgeblich beeinflussen sowie für die am Anfang genannten Testunteschiede sorgt.

\section{Vergleich mit manuellen Testergebnissen}