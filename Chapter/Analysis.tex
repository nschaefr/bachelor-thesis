\chapter{Durchführung und Analyse}\label{chap:analysis}
In diesem Kapitel erfolgt die Durchführung der Generierung von Tests mit \textit{Unitcraft} anhand verschiedener Projekte. Im Anschluss findet eine Analyse der Ergebnisse statt, welche auf Basis zuvor definierter Metriken und eines in Kapitel \ref{section:eval} beschriebenen Verfahrens durchgeführt wird. Es findet eine Codeanalyse statt, welche auf Unterschiede in den Tests eingeht und anhand der Ergebnisse Schlüsse zieht. Abschließend werden die erzeugten Metriken mit den Metriken der menschlichen Tester verglichen und das Gesamtergebnis von \textit{Unitcraft} eingeordnet.

\section{Durchführung}
Die Durchführung findet an einer Auswahl von Projekten mit unterschiedlichen Komplexitätsgraden und Größen statt. Die Projekte entstammen dem Unternehmen SENEC GmbH und werden mit ``Projekt x'' (x für eine Zahl) benannt. Projekt 1 besitzt eine Komplexität von 33 und ist mit einer Anzahl an Codezeilen von 318 das kleinste Projekt. Es folgt Projekt 2 mit einer Komplexität von 102 und einer Codezeilenanzahl von 730. Das größte und komplexeste Projekt ist Projekt 3. Die Komplexität beträgt 124 und die Zeilen an Quellcode 1231. [Tabelle \ref{fig:project}]\\\input{Assets/LaTeX/Project-Table}Für jedes Projekt werden 6 Testdurchläufe durchgeführt. Diese entstehen durch die Kombination von \textit{Prompt}-Technik und dem Temperaturparameter. Somit wird der \textit{Zero-Shot Prompt} sowie der \textit{One-Shot Prompt} mit der Temperatur von 0, 0.25 und 0.5 für die Generierung verwendet. Insgesamt erzielen wir 18 Testdurchläufe, welche dafür sorgen, dass eine aussagekräftige Anzahl an Tests generiert wird.

\section{Evaluation der Tests}\label{section:eval}
Zur Bewertung und Analyse der Testergebnisse werden folgende Metriken benutzt: 
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item \textit{Line Coverage}
    \item \textit{Branch Coverage}
    \item \textit{Overall Coverage}
    \item Erfolgsquote mit Anzahl an \textit{Unit Tests}
\end{itemize}
Jede Metrik wird projektweise evaluiert und bildet Durchschnittswerte aus allen Projekten. Anhand der Durchschnittswerte werden die Ergebnisse in Kategorien eingegliedert. Eine Einteilung der \textit{Coverage} nach bestimmten Prozentwerten ist nicht sinnvoll, da es keinen Wert gibt, der auf alle Projekte zutrifft. Aufgrund dessen wird der Prozentbereich von 60\% - 90\% definiert und unterschieden, ob die \textit{Coverage} in diesem angestrebten Bereich liegt (grün markiert) oder nicht (rot markiert). \cite{WhatReasonableCode} Zusätzlich zur Ermittlung der Durchschnittswerte wird die Standardabweichung der Messwerte berechnet, um die Konsistenz der Ergebnisse zwischen den verschiedenen Projekten zu bewerten. Dazu wird eine Berechnung  Varianz über folgende Formel vorausgesetzt:\\\\
$s^2 = \frac{1}{n-1} \displaystyle\sum_{i=1}^{n} (x_i - \bar{x})^2$
\begin{itemize}
    \setlength{\parskip}{1pt}
    \item $n$ - Anzahl der Stichproben
    \item $x_i$ - Wert der einzelnen Stichprobe
    \item $\bar{x}$ - Mittelwert/Durchschnitt der Stichproben
\end{itemize}
Ist die Varianz berechnet, so kann die Standardabweichung über die Wurzel dieser ermittelt werden: 
$s=\sqrt{s^2}$


\subsection{Line Coverage}
Tabelle \ref{fig:line-1} zeigt die Ergebnisse der \textit{Line Coverage} für Projekt 1. \\Den besten Wert mit 75.3\% erzielt der \textit{Zero-Shot Prompt} in Kombination mit einer Temperatur von 0.25. Der \textit{Zero-Shot Prompt} mit einer Temperatur von 0.5 schneidet mit 62.5\% am schlechtesten ab.
\input{Assets/LaTeX/Line-Cov-1.tex}
Die Ergebnisse der \textit{Line Coverage} für Projekt 2 sind in Tabelle \ref{fig:line-2} dargestellt. \\Mit 84.9 \% erreicht der \textit{One-Shot Prompt} mit einer Temperatur von 0.25 den höchsten Wert. Der \textit{Zero-Shot Prompt} mit Temperatur 0 schneidet mit über 20\% schlechter ab.
\input{Assets/LaTeX/Line-Cov-2.tex}
Tabelle \ref{fig:line-3} stellt die Ergebnisse der \textit{Line Coverage} für Projekt 3 dar. \\Mit 62.7\% erzielt der \textit{One-Shot Prompt} mit einer Temperatur von 0 den besten Wert. Der schlechteste Wert ist 47.3\% und entsteht in der Kombination aus \textit{One-Shot Prompt} und einer Temperatur von 0.5.
\input{Assets/LaTeX/Line-Cov-3.tex}
In Tabelle \ref{fig:line-avg} ist der Durchschnitt aller Ergebnisse dargestellt.\\ Alle Werte treffen den angestrebten Bereich von 60\% - 90\%. Den besten Wert erreicht der \textit{One-Shot Prompt} mit einer Temperatur von 0.25. Am schlechtesten schneidet der \textit{Zero-Shot Prompt} mit einer Temperatur von 0.5 ab. \\Es ist klar erkennbar, dass eine hohe Temperatur wie 0.5, im Vergleich zum Rest deutlich schlechter abschneidet. Auch der \textit{One-Shot Prompt} erreicht im Vergleich zum \textit{Zero-Shot Prompt} die besseren Ergebnisse. 
\input{Assets/LaTeX/Line-Cov-avg.tex}
Tabelle \ref{fig:line-var} zeigt die Standardabweichung der obigen Messwerte.\\ Die größte Abweichung entsteht in der Kombination aus \textit{Zero-Shot Prompt} und der Temperatur 0. Mit einer Abweichung von 6.6 ist der \textit{Zero-Shot Prompt} mit der Temperatur 0.5 am konsistentesten.\\ In allen Fällen verursacht der Leistungseinbruch in Projekt 3 die größte Abweichung.
\input{Assets/LaTeX/Line-Cov-var.tex}

\subsection{Branch Coverage}
Wie in Tabelle \ref{fig:branch-1} dargestellt, beträgt die \textit{Branch Coverage} für Projekt 1 in allen Kombinationen 50.0\%. Somit schneidet jeder Prompt gleich ab.
\input{Assets/LaTeX/Branch-Cov-1.tex}
In Tabelle \ref{fig:branch-2} erreicht der \textit{One-Shot Prompt} mit einer Temperatur von 0.25 erneut das beste Ergebnis der \textit{Branch Coverage} für Projekt 2 und sticht im Vergleich zum Rest klar hervor.
\input{Assets/LaTeX/Branch-Cov-2.tex}
Für Projekt 3 fallen die Werte erheblich schlechter aus. Der \textit{One-Shot Prompt} erzielt mit 32.0\% und einer Temperatur von 0 das beste Ergebnis. Die Temperatur von 0.5 schneidet am schlechtesten ab.
\input{Assets/LaTeX/Branch-Cov-3.tex}
Tabelle \ref{fig:branch-avg} zeigt die Durchschnittswerte der \textit{Branch Coverage}. \\Es ist eine klare Verschlechterung im Vergleich zur \textit{Line Coverage} zu erkennen. Im Schnitt erreicht der \textit{One-Shot Prompt} mit der Temperatur 0.25 erneut den besten Wert. Auch der \textit{Zero-Shot Prompt} schneidet in Kombination mit einer Temperatur von 0.5 am schlechtesten ab.\\ Des Weiteren wird deutlich, dass die Temperatur von 0.5 die schlechtesten Werte erzielt. Der Vergleich der \textit{Prompts} ist ausgeglichener als zuvor, jedoch liegen alle Ergebnisse unter dem vorher festgelegten Idealbereich. 
\input{Assets/LaTeX/Branch-Cov-avg.tex}
Der \textit{One-Shot Prompt} erzielt mit einer Temperatur von 0 die beste Standardabweichung von 10.7. Am schlechtesten schneidet der \textit{Zero-Shot Prompt} mit Temperatur 0 ab. Auch hier wird deutlich, dass die Verschlechterung in Projekt 3 den größten Einfluss auf das Ergebnis nimmt. [Tabelle \ref{fig:branch-var}]
\input{Assets/LaTeX/Branch-Cov-var.tex}

\subsection{Overall Coverage}
Tabelle \ref{fig:o-1} stellt die \textit{Overall Coverage} Ergebnisse für Projekt 1 dar. \\72.4\% ist der Höchstwert, erreicht durch die Kombination aus \textit{Zero-Shot} und der Temperatur 0.25. Die Temperatur 0.5 liefert wie schon bei der \textit{Line Coverage}  die schlechtesten Ergebnisse.\\ Betrachtet man die Werte im Gesamten, erreicht das Sprachmodell ein zufriedenstellendes Ergebnis, da kein Wert unter 60\% fällt.
\input{Assets/LaTeX/O-Cov-1.tex}
Mit einer \textit{Overall Coverage}  von 81.0\% erzielt die Kombination aus \textit{One-Shot Prompt} und einer Temperatur von 0.25 den besten Wert für Projekt 2. Der schlechteste Wert beträgt 59.0\% und liegt somit unter der 60\% Grenze.\\ Der Rest erreicht ein zufriedenstellendes Ergebnis. [Tabelle \ref{fig:o-2}]
\input{Assets/LaTeX/O-Cov-2.tex}
Eine klare Verschlechterung erkennt man im komplexesten und größten Projekt, dem Projekt 3. Tabelle \ref{fig:o-3} stellt die Ergebnisse tabellarisch dar. Es wird direkt erkennbar, dass keine Kombination die 60\% erreicht, und der höchste Wert bei einer \textit{Coverage} von 53.5\% liegt. Hier wird deutlich, dass sich die Ergebnisse des Sprachmodells bei zunehmender Komplexität verschlechtern.
\input{Assets/LaTeX/O-Cov-3.tex}
Tabelle \ref{fig:o-avg} zeigt die Durchschnittswerte und verdeutlicht ein zufriedenstellendes Ergebnis, da 3 von 6 Ergebnissen den angestrebten Bereich erreichen. Wie auch in den vorherigen Metriken, erzielt die Kombination aus \textit{One-Shot Prompt} und der Temperatur 0.25 den besten Wert. Die Temperatur von 0.5 erlangt die schlechtesten Ergebnisse und der \textit{One-Shot Prompt} ist dem \textit{Zero-Shot Prompt} sichtbar überlegen.
\input{Assets/LaTeX/O-Cov-avg.tex}
In Tabelle \ref{fig:o-var} ist die Standardabweichung aller Messwerte zu den zugehörigen Kombinationen zu erkennen. Es wird schnell deutlich, dass alle Werte in einem Bereich von 9.0 - 21.6 liegen, was auf eine teilweise inkonsistente Leistung des Sprachmodells bei verschiedenen Komplexitätsgraden hinweist. Betrachtet man die Tabellen genauer, ist ein starker Leistungsabfall im Projekt 3 zu erkennen, was die eben genannte Aussage noch einmal vestärkt und die teilweise erhöhten Abweichungen begründet. Die konsistenteste Leistung weist der \textit{One-Shot Prompt} in Kombination mit einer Temperatur von 0 auf. Am inkonsistentesten ist der \textit{Zero-Shot Prompt} mit einer Temperatur von 0.
\input{Assets/LaTeX/Overall-Cov-var.tex}

\subsection{Erfolgsquote}
Da die Erfolgsquote in allen Projekten ähnliche Werte aufweist und somit keine klare Unterscheidung aufgrund von Komplexität und Größe des Projektes getroffen werden kann, wird zur Evaluation dieser Quote der durchschnittliche Erfolg, auf Basis aller Projekte verwendet.\\
In Tabelle \ref{fig:succ} ist erkennbar, dass die Erfolgsquote zwischen 42.7\% (\textit{Zero-Shot Prompt} mit Temperatur von 0.5) und 46.0\% (\textit{One-Shot Prompt} mit Temperatur von 0.25) liegt. \input{Assets/LaTeX/Success.tex}Die in Tabelle \ref{fig:unit} dargestellten durchschnittlich erzeugten \textit{Unit Tests} dienen der Berechnung der tatsächlich erfolgreichen Tests.\input{Assets/LaTeX/Unit.tex}Dabei multiplizieren wir die Anzahl der \textit{Unit Tests} mit der Erfolgsquote. Das Ergebnis in Tabelle \ref{fig:succ-unit} stellt die tatsächlich erfolgreichen  \textit{Unit Tests} dar, die nicht aufgrund eines \textit{Errors} während der Ausführung fehlschlugen. \input{Assets/LaTeX/Succ-Unit.tex} Es wird deutlich, dass weniger als die Hälfte der generierten  \textit{Unit Tests} erfolgreich ausgeführt werden. Dadurch entsteht ein großer Anteil an irrelevantem Code im Projekt, was die Quellcode- sowie Projektqualität erheblich verschlechtert. Aus einem anderen Blickwinkel wird ebenso Potenzial für die Testabdeckung verschwendet, wenn ein  \textit{Unit Test} aufgrund eines simplen \textit{Errors} wie bspw. einer \textit{NullPointerException} fehlerhaft ist.

\section{Testcodeanalyse}
Tabelle \ref{fig:succ} verdeutlicht, dass sich die Erfolgsquote aller Kombinationen aus \textit{Prompt}-Technik und Temperatur kaum unterscheiden. Daraus lässt sich erschließen, dass die Unterschiede in den \textit{Coverage}-Ergebnissen nicht aus dem Erfolg der generierten Tests ableitbar sind. Eine genauerer Blick auf den Testcode ist notwendig, um Ergebnisse der Analyse begründen zu können. Dazu wird der generierte Testcode mit der im Schnitt schlechtesten \textit{Overall Coverage}  (\textit{Zero-Shot Prompt} mit Temperatur von 0.5) sowie der besten \textit{Overall Coverage}  (\textit{One-Shot Prompt} mit Temperatur von 0.25) verglichen, um Unterschiede nachvollziehen zu können. Es wird deutlich, dass fehlende \textit{Packages}, \textit{Imports} und \textit{Private-Access-Errors} die stärksten Hürden darstellen und einen Kompilierfehler erzwingen, sodass der generierte Test von \textit{Unitcraft} gelöscht wird. \\\\
Quellcode \ref{lst:missing-package} zeigt ein solches Vergessen des \textit{Packages}, welches ein \textit{Missing package statement: 'com.senec.emsrelay'} verursacht und somit ein erfolgreiches Kompilieren des Tests verhindert.\\
\lstinputlisting[caption=Fehlendes \textit{Package} in Testklasse,captionpos=b,label={lst:missing-package},language=Java]{Assets/Code/Missing-Package.java}
In Quellcode \ref{lst:missing-import} ist ein fehlender Import dargestellt, welcher ebenso einen Kompilierfehler verursacht, da die Methode \textit{assertEquals} somit nicht bereitgestellt und das Löschen der gesamten Testklasse erzwungen wird.\\
\lstinputlisting[caption=Fehlender Import in Testklasse,captionpos=b,label={lst:missing-import},language=Java]{Assets/Code/Missing-Import.java}
Obwohl alle \textit{Prompts} so designt sind, dass die Verwendung von \textit{Reflection} für den Zugriff auf private Methoden oder Attribute empfohlen wird, ist vermehrt sichtbar, dass bei der Wahl der Temperatur von 0.5 diese Anweisung ignoriert und somit durch einen \textit{Privat-Access-Error} die Ausführung des Tests verhindert wird.\\
In Abbildung \ref{fig:temp} wird der Einfluss der Temperatur in Form einer Verschlechterung bei einem Anstieg über 0.25 erkennbar. \input{Assets/LaTeX/Temperature}Betrachtet man die oben genannten Unterschiede im Testcode im Zusammenhang mit Abbildung \ref{fig:temp} kann man schlussfolgern, dass durch eine höhere Temperatur und somit einer höheren Kreativität der Fokus auf wichtige Aspekte verloren geht. Eine deterministischer Wert sorgt für bessere Ergebnisse in der \textit{Coverage}. Die Auswertung der Daten in Tabelle \ref{fig:o-avg} zeigt jedoch, dass die Temperatur 0.25 in Kombination mit dem \textit{One-Shot Prompt} die beste \textit{Coverage} erzielt. Dies verdeutlicht, dass durch einen geringen Anteil an Kreativität und mit zusätzlichem Beispielkontext durch die \textit{Prompt}-Technik, unerwartete Hürden wie beispielsweise der Zugriff auf private Methoden oder zusätzlich notwendige \textit{Imports}, besser gelöst werden können. Der Beispielkontext kann den Fokus auf wichtige Aspekte, trotz eines geringen Anteils an Kreativität, aufrechterhalten.\\
Somit nimmt nicht nur die Temperatur Einfluss auf die Lösung oben genannter Fehlerursachen. Abbildung \ref{fig:prompt} zeigt, dass der \textit{One-Shot Prompt} bessere Ergebnisse erzielt, als der \textit{Zero-Shot Prompt} \input{Assets/LaTeX/Prompt}Daraus lässt sich ableiten, dass durch das Hinzufügen eines Beispielkontextes innerhalb eines \textit{One-Shot Prompt}s die oben genannten Hürden besser gelöst werden. Dies ist auf den zusätzlichen Kontext zurückführen, da dieser durch beispielhaft korrektes Einbinden von \textit{Imports} und \textit{Packages} eine Vorlage bietet.\\\\
Schlussfolgernd lässt sich sagen, dass beide Faktoren das Ergebnis maßgeblich beeinflussen sowie für die am Anfang genannten Testunterschiede sorgen. Der \textit{One-Shot Prompt} ist aufgrund seines zusätzlichen Kontextes dem \textit{Zero-Shot Prompt} überlegen. Vor allem in Kombination mit einer deterministischen Temperatur mit einem Anteil von Kreativität, welcher 0.25 nicht überschreitet, wurden die besten Ergebnisse erzielt. Die richtige Wahl von \textit{Prompt}-Technik sowie Temperatur ist essenziell für die Lösung oben genannter Probleme.

\section{Vergleich mit manuellen Testergebnissen}
Im Folgenden werden die Testergebnisse zwischen den von \textit{Unitcraft} generierten Tests und denen, die in der Softwareentwicklung der SENEC GmbH manuell erstellt wurden, verglichen. Dabei wird das jeweilige beste Ergebnis des Sprachmodells genutzt, um das mögliche Potenzial im Vergleich zu den vom Menschen geschriebenen Tests aufzuzeigen.\\
Tabelle \ref{fig:comp} zeigt den direkten Vergleich der Metriken tabellarisch.\input{Assets/LaTeX/Comp.tex}Im Projekt 1 erreichen die manuell geschriebenen Tests mit 84.4\% die bessere \textit{Line Coverage}. Betrachtet man jedoch Projekt 2 und 3, wird deutlich, dass \textit{GPT-4o} klar bessere Ergebnisse liefert. Speziell im 2. Projekt erreicht das Sprachmodell eine mehr als doppelt so gute Leistung. Beim Vergleich der \textit{Branch Coverage} schneiden die Testergebnisse der manuellen Tester in allen Projekten schlechter ab. Auch hier sticht \textit{GPT-4o} in Projekt 2 mit einer besonderen Leistung hervor. Ähnlich wie bei der \textit{Line Coverage} erreichen die manuellen Tests in Projekt 1 knapp 7\% mehr \textit{Overall Coverage}. Vergleicht man hierbei wieder mit Projekt 2 und 3 wird stark deutlich, dass \textit{GPT-4o} besser performt. Wie schon in Tabelle \ref{fig:succ} analysiert, schneidet die Erfolgsquote des Sprachmodells schlecht ab. Insbesondere im direkten Vergleich entsteht eine große Differenz zwischen beiden Werten. Die manuellen Tests weisen eine Erfolgsquote von 100\% in jedem Projekt auf. Dies ist ein zu erwartender Wert, da beim manuellen Testen nur Tests im Projekt bleiben, die erfolgreich ausgeführt werden. Somit wird irrelevanter Testcode vermieden und die Qualität im Projekt gesichert.\\\\
Bei einer Gesamtbetrachtung aller Vergleiche zeigt sich, dass \textit{GPT-4o} in 7 von 12 Metriken bessere Werte aufweist. Dies veranschaulicht das Potenzial des Sprachmodells beim Generieren von \textit{JUnit Tests}. Es wird jedoch auch deutlich, dass der Testcode von manuellen Tests eindeutig erfolgreicher in der Auführung und somit qualitativer ist, da im Schnitt weniger irrelevanter Testcode entsteht.

\section{Einordnung der Ergebnisse}
Betrachtet man die Ergebnisse im Gesamten, erreicht \textit{Unitcraft} und somit \textit{GPT-4o} ein zufriedenstellendes Ergebnis. In 2 von 3 Projekten erreicht das \textit{Tool} eine \textit{Overall Coverage} von über 60\%. Vorallem im Vergleich zu manuellen Testergebnissen der SENEC GmbH sticht \textit{Unitcraft} mit einer starken \textit{Performance} hervor. Bei der Analyse der Erfolgsquote wurde jedoch auch deutlich, dass in den vom Sprachmodell generierten Tests ein großer Anteil an irrelevantem Code ensteht, welcher die Qualität im Projekt maßgeblich beeinflusst und somit als nicht zufriedenstellend deklariert werden muss.

