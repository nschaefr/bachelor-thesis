@online{hammermann_ki_nodate,
	title = {{KI} und der Arbeitsmarkt: Eine Analyse der Beschäftigungseffekte. Ein Überblick über aktuelle empirische Befunde},
	author = {Hammermann, Andrea and Monsef, Roschan and Stettes, Oliver},
	url = {https://www.econstor.eu/bitstream/10419/279538/1/1867585626.pdf},
	langid = {german},
	file = {Hammermann et al. - KI und der Arbeitsmarkt Eine Analyse der Beschäft.pdf:/home/nschaef/Zotero/storage/W8YU5ZNL/Hammermann et al. - KI und der Arbeitsmarkt Eine Analyse der Beschäft.pdf:application/pdf},
	urldate = {2024-06-04},
}

@online{noauthor_was_nodate,
	title = {Was ist {SDLC}? – Softwareentwicklungs-Lebenszyklus erläutert – {AWS}},
	url = {https://aws.amazon.com/de/what-is/sdlc/},
	shorttitle = {Was ist {SDLC}?},
	abstract = {Was ist {SDLC}, wie und warum verwenden Unternehmen {SDLC} und wie wird {SDLC} mit {AWS} verwendet.},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2024-06-04},
	langid = {german},
	file = {Snapshot:/home/nschaef/Zotero/storage/TNIJ559X/sdlc.html:text/html},
}

@article{pargaonkar_study_2023,
	title = {A Study on the Benefits and Limitations of Software Testing Principles and Techniques: Software Quality Engineering},
	volume = {13},
	issn = {22503153},
	url = {https://www.ijsrp.org/research-paper-0823.php?rp=P14013002},
	doi = {10.29322/IJSRP.13.08.2023.p14018},
	shorttitle = {A Study on the Benefits and Limitations of Software Testing Principles and Techniques},
	pages = {149--155},
	number = {8},
	journaltitle = {International Journal of Scientific and Research Publications},
	shortjournal = {{IJSRP}},
	author = {Pargaonkar, Shravan},
	urldate = {2024-06-04},
	date = {2023-08-24},
}

@article{garousi_survey_2013,
	title = {A survey of software testing practices in Canada},
	volume = {86},
	issn = {0164-1212},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121212003561},
	doi = {10.1016/j.jss.2012.12.051},
	abstract = {Software testing is an important activity in the software development life-cycle. In an earlier study in 2009, we reported the results of a regional survey of software testing practices among practitioners in the Canadian province of Alberta. To get a larger nationwide view on this topic (across Canada), we conducted a newer survey with a revised list of questions in 2010. Compared to our previous Alberta-wide survey (53 software practitioners), the nation-wide survey had larger number of participants (246 practitioners). We report the survey design, execution and results in this article. The survey results reveal important and interesting findings about software testing practices in Canada. Whenever possible, we also compare the results of this survey to other similar studies, such as the ones conducted in the {US}, Sweden and Australia, and also two previous Alberta-wide surveys, including our 2009 survey. The results of our survey will be of interest to testing professionals both in Canada and world-wide. It will also benefit researchers in observing the latest trends in software testing industry identifying the areas of strength and weakness, which would then hopefully encourage further industry-academia collaborations in this area. Among the findings are the followings: (1) the importance of testing-related training is increasing, (2) functional and unit testing are two common test types that receive the most attention and efforts spent on them, (3) usage of the mutation testing approach is getting attention among Canadian firms, (4) traditional Test-last Development ({TLD}) style is still dominating and a few companies are attempting the new development approaches such as Test-Driven Development ({TDD}), and Behavior-Driven Development ({BDD}), (5) in terms of the most popular test tools, {NUnit} and Web application testing tools overtook {JUnit} and {IBM} Rational tools, (6) most Canadian companies use a combination of two coverage metrics: decision (branch) and condition coverage, (7) number of passing user acceptance tests and number of defects found per day (week or month) are regarded as the most important quality assurance metrics and decision factors to release, (8) in most Canadian companies, testers are out-numbered by developers, with ratios ranging from 1:2 to 1:5, (9) the majority of Canadian firms spent less than 40\% of their efforts (budget and time) on testing during development, and (10) more than 70\% of respondents participated in online discussion forums related to testing on a regular basis.},
	pages = {1354--1376},
	number = {5},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Garousi, Vahid and Zhi, Junji},
	urldate = {2024-06-04},
	date = {2013-05-01},
	keywords = {Canada, Industry practices, Software testing, Survey},
	file = {ScienceDirect Snapshot:/home/nschaef/Zotero/storage/GYQC75P2/S0164121212003561.html:text/html},
}

@misc{poldrack_ai-assisted_2023,
	title = {{AI}-assisted coding: Experiments with {GPT}-4},
	url = {http://arxiv.org/abs/2304.13187},
	shorttitle = {{AI}-assisted coding},
	abstract = {Artiﬁcial intelligence ({AI}) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using {GPT}-4 to generate computer code. These experiments demonstrate that {AI} code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that {GPT}-4 refactoring of existing code can signiﬁcantly improve that code along several established metrics for code quality, and we show that {GPT}-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These ﬁndings suggest that while {AI} coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.},
	number = {{arXiv}:2304.13187},
	publisher = {{arXiv}},
	author = {Poldrack, Russell A. and Lu, Thomas and Beguš, Gašper},
	urldate = {2024-06-04},
	date = {2023-04-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.13187 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Poldrack et al. - 2023 - AI-assisted coding Experiments with GPT-4.pdf:/home/nschaef/Zotero/storage/QHL8KP8P/Poldrack et al. - 2023 - AI-assisted coding Experiments with GPT-4.pdf:application/pdf},
}

@online{hossainSoftwareDevelopmentLife2023,
  title = {Software {{Development Life Cycle}} ({{SDLC}}) {{Methodologies}} for {{Information Systems Project Management}}},
  author = {Hossain, Mohammad},
  date = {2023-09-09},
  doi = {10.36948/ijfmr.2023.v05i05.6223},
  abstract = {Abstract: The software development life cycle (SDLC) is a framework for planning, analyzing, designing, developing, testing, and deploying software. There are many different SDLC methodologies available, each with its advantages and disadvantages. The best methodology for a particular project will depend on factors such as the size and complexity of the project, the availability of resources, and the preferences of the project team. This paper provides an in-depth overview of the different SDLC methodologies used in the industry for IS project management. The paper discusses five significant SDLC methodologies: Waterfall, V-Model, Iterative, Agile, and Hybrid. It also compares and contrasts the traditional SDLC and Agile methodology and discusses using decision support matrices for selecting SDLC methodologies. By providing this comprehensive overview, the paper will help IS practitioners and project managers make informed decisions about the best approach for their specific projects.},
  file = {/home/nschaef/Zotero/storage/HVNLIVDK/Hossain - 2023 - Software Development Life Cycle (SDLC) Methodologi.pdf}
}

@online{SoftwareDevelopmentLife,
  title = {Software {{Development Life Cycle}} ({{SDLC}})},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/ZXHE44PS/Software Development Life Cycle (SDLC).pdf},
  url = {https://www.sncwgs.ac.in/wp-content/uploads/2015/11/sdlc_tutorial.pdf}
}

@online{tikySoftwareDevelopmentLife,
  title = {Software {{Development Life Cycle}}},
  author = {Tiky, Yan Ting Wong},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/WYBBAST3/Tiky - Software Development Life Cycle.pdf},
  url = {https://home.cse.ust.hk/~rossiter/independent_studies_projects/software_development/software_development_report.pdf}
}

@article{authorityofthehouseoflordsLargeLanguageModels,
  title = {Large Language Models and Generative {{AI}}},
  author = {{Authority of the House of Lords}},
  url = {https://publications.parliament.uk/pa/ld5804/ldselect/ldcomm/54/54.pdf},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/FAN5W9A9/Large language models and generative AI.pdf}
}

@book{cholletDeepLearningPython2018,
  title = {Deep Learning with {{Python}}},
  author = {Chollet, François},
  date = {2018},
  publisher = {Manning Publications Co},
  location = {Shelter Island, New York},
  isbn = {978-1-61729-443-3},
  langid = {english},
  pagetotal = {361},
  keywords = {Machine learning,Neural networks (Computer science),Python (Computer program language)},
  annotation = {OCLC: ocn982650571},
  file = {/home/nschaef/Zotero/storage/PVQTSXXG/Chollet - 2018 - Deep learning with Python.pdf}
}

@online{dhadukHowLargeLanguage2023,
  title = {How {{Do Large Language Models Work}}?},
  author = {Dhaduk, Hiren},
  date = {2023-04-21T09:07:44+00:00},
  url = {https://www.simform.com/blog/how-do-llm-work/},
  urldate = {2024-06-13},
  abstract = {Know everything about large language models right from their types, examples, applications and how they work},
  langid = {american},
  organization = {Simform - Product Engineering Company},
  file = {/home/nschaef/Zotero/storage/IVT6WXL4/how-do-llm-work.html}
}

@inproceedings{duEvaluatingLargeLanguage2024,
  title = {Evaluating {{Large Language Models}} in {{Class-Level Code Generation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
  date = {2024-04-12},
  pages = {1--13},
  publisher = {ACM},
  location = {Lisbon Portugal},
  doi = {10.1145/3597503.3639219},
  url = {https://dl.acm.org/doi/10.1145/3597503.3639219},
  urldate = {2024-06-16},
  abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios. To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., classlevel code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the methodlevel. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers’ expertise to build more LLM benchmarks based on practical and complicated software development scenarios.},
  eventtitle = {{{ICSE}} '24: {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  isbn = {9798400702174},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/YV7KS7PU/Du et al. - 2024 - Evaluating Large Language Models in Class-Level Co.pdf}
}

@online{HowLLMsWork,
  title = {How Do {{LLMs}} Work? {{Key}} Concepts of {{Generative AI}} | {{CloudX}}},
  url = {https://cloudx.com/blog/how-do-large-language-models-work-key-concepts-of-generative-ai-you-should-know},
  urldate = {2024-06-13},
  file = {/home/nschaef/Zotero/storage/KBUQ8XZN/how-do-large-language-models-work-key-concepts-of-generative-ai-you-should-know.html}
}

@online{WeightsBiases,
  title = {Weights \& {{Biases}}},
  url = {https://wandb.ai/darek/llmapps/reports/A-Gentle-Introduction-to-LLM-APIs--Vmlldzo0NjM0MTMz},
  urldate = {2024-06-16},
  abstract = {Weights \& Biases, developer tools for machine learning},
  langid = {english},
  organization = {W\&B},
  file = {/home/nschaef/Zotero/storage/LZZ97UNX/A-Gentle-Introduction-to-LLM-APIs--Vmlldzo0NjM0MTMz.html}
}

@article{morrKuenstlicheNeuronaleNetze,
  title = {Künstliche neuronale Netze},
  author = {Morr, Sebastian and Ebeling, Herr},
  langid = {ngerman},
  file = {/home/nschaef/Zotero/storage/3E7S3C75/Morr and Ebeling - Ku¨nstliche neuronale Netze.pdf}
}

@unpublished{muruganNaturalLanguageProcessing2024,
  title = {Natural {{Language Processing}} ({{NLP}})},
  author = {Murugan, Mohana},
  date = {2024-04-03},
  doi = {10.13140/RG.2.2.13534.04169},
  abstract = {Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on enabling computers to interact, understand, interpret, and generate human language. It encompasses a wide range of techniques and algorithms designed to process and analyze natural language data, such as text and speech. Learning NLP is crucial in today's digital era for several reasons. Firstly, it empowers businesses and organizations to extract valuable insights from large volumes of unstructured textual data, aiding in decision-making, market analysis, and customer feedback analysis. Secondly, NLP plays a pivotal role in developing advanced communication systems, including chatbots, virtual assistants, and language translation tools, which enhance user experience and streamline interactions between humans and machines. Moreover, NLP facilitates sentiment analysis, topic modeling, text summarization, and information retrieval, making it indispensable in fields such as healthcare, education, finance, legal, and e-commerce. Overall, mastering NLP opens up a world of opportunities to harness the power of language for innovation, efficiency, and a better understanding of human communication patterns and behaviors.},
  file = {/home/nschaef/Zotero/storage/ASTIX7LI/Murugan - 2024 - Natural Language Processing (NLP).pdf}
}

@online{naveedComprehensiveOverviewLarge2024,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  date = {2024-04-09},
  eprint = {2307.06435},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.06435},
  urldate = {2024-06-13},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/nschaef/Zotero/storage/WLNAL9EK/Naveed et al. - 2024 - A Comprehensive Overview of Large Language Models.pdf}
}

@online{NeuronaleNetzeEinfuhrung,
  title = {Neuronale {{Netze}} - {{Eine Einführung}} - {{Grundlagen}}},
  url = {http://www.neuronalesnetz.de/},
  urldate = {2024-06-13},
  file = {/home/nschaef/Zotero/storage/L35VZFEG/www.neuronalesnetz.de.html}
}

@article{okerlundWhatChatterbox2022,
  title = {What’s in the {{Chatterbox}}?},
  author = {Okerlund, Johanna and Klasky, Evan and Middha, Aditya and Kim, Sujin and Rosenfeld, Hannah and Kleinman, Molly and Parthasarathy, Shobita},
  date = {2022},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/Q98VZ68P/Okerlund et al. - 2022 - What’s in the Chatterbox.pdf}
}

@online{OpenAIPlatform,
  title = {{{OpenAI Platform}}},
  url = {https://platform.openai.com},
  urldate = {2024-06-16},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/K4DX8KJM/gpt-4-turbo-and-gpt-4.html}
}

@article{seemannKuenstlicheIntelligenzLarge,
  title = {Künstliche Intelligenz, Large Language Models, ChatGPT und die Arbeitswelt der Zukunft},
  author = {Seemann, Michael},
  langid = {ngerman},
  file = {/home/nschaef/Zotero/storage/Y7RUUTJV/Seemann - Künstliche Intelligenz, Large Language Models, Cha.pdf}
}

@online{shahandashtiEvaluatingEffectivenessGPT42024,
  title = {Evaluating the {{Effectiveness}} of {{GPT-4 Turbo}} in {{Creating Defeaters}} for {{Assurance Cases}}},
  author = {Shahandashti, Kimya Khakzad and Sivakumar, Mithila and Mohajer, Mohammad Mahdi and Belle, Alvine B. and Wang, Song and Lethbridge, Timothy C.},
  date = {2024-01-31},
  eprint = {2401.17991},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.17991},
  url = {http://arxiv.org/abs/2401.17991},
  urldate = {2024-06-16},
  abstract = {Assurance cases (ACs) are structured arguments that support the verification of the correct implementation of systems' non-functional requirements, such as safety and security, thereby preventing system failures which could lead to catastrophic outcomes, including loss of lives. ACs facilitate the certification of systems in accordance with industrial standards, for example, DO-178C and ISO 26262. Identifying defeaters arguments that refute these ACs is essential for improving the robustness and confidence in ACs. To automate this task, we introduce a novel method that leverages the capabilities of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, to identify defeaters within ACs formalized using the Eliminative Argumentation (EA) notation. Our initial evaluation gauges the model's proficiency in understanding and generating arguments within this framework. The findings indicate that GPT-4 Turbo excels in EA notation and is capable of generating various types of defeaters.},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering},
  file = {/home/nschaef/Zotero/storage/Q9337LDB/Shahandashti et al. - 2024 - Evaluating the Effectiveness of GPT-4 Turbo in Cre.pdf;/home/nschaef/Zotero/storage/6NNB5FA9/2401.html}
}

@article{yaoSurveyLargeLanguage2024,
  title = {A Survey on Large Language Model ({{LLM}}) Security and Privacy: {{The Good}}, {{The Bad}}, and {{The Ugly}}},
  shorttitle = {A Survey on Large Language Model ({{LLM}}) Security and Privacy},
  author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  date = {2024-06-01},
  journaltitle = {High-Confidence Computing},
  shortjournal = {High-Confidence Computing},
  volume = {4},
  number = {2},
  pages = {100211},
  issn = {2667-2952},
  doi = {10.1016/j.hcc.2024.100211},
  url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
  urldate = {2024-06-16},
  abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.},
  keywords = {ChatGPT,Large Language Model (LLM),LLM attacks,LLM privacy,LLM security,LLM vulnerabilities},
  file = {/home/nschaef/Zotero/storage/RSYZ4GBK/Yao et al. - 2024 - A survey on large language model (LLM) security an.pdf;/home/nschaef/Zotero/storage/7HNS82XZ/S266729522400014X.html}
}

@online{yeComprehensiveCapabilityAnalysis2023,
  title = {A {{Comprehensive Capability Analysis}} of {{GPT-3}} and {{GPT-3}}.5 {{Series Models}}},
  author = {Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and Zhou, Jie and Chen, Siming and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  date = {2023-12-23},
  eprint = {2303.10420},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.10420},
  url = {http://arxiv.org/abs/2303.10420},
  urldate = {2024-06-16},
  abstract = {GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on, have gained considerable attention due to their exceptional natural language processing capabilities. However, despite the abundance of research on the difference in capabilities between GPT series models and fine-tuned models, there has been limited attention given to the evolution of GPT series models' capabilities over time. To conduct a comprehensive analysis of the capabilities of GPT series models, we select six representative models, comprising two GPT-3 series models (i.e., davinci and text-davinci-001) and four GPT-3.5 series models (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo). We evaluate their performance on nine natural language understanding (NLU) tasks using 21 datasets. In particular, we compare the performance and robustness of different models for each task under zero-shot and few-shot scenarios. Our extensive experiments reveal that the overall ability of GPT series models on NLU tasks does not increase gradually as the models evolve, especially with the introduction of the RLHF training strategy. While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/nschaef/Zotero/storage/JXPCYH54/Ye et al. - 2023 - A Comprehensive Capability Analysis of GPT-3 and G.pdf;/home/nschaef/Zotero/storage/LQG4B8XQ/2303.html}
}

@online{yenduriGenerativePretrainedTransformer2023,
  title = {Generative {{Pre-trained Transformer}}: {{A Comprehensive Review}} on {{Enabling Technologies}}, {{Potential Applications}}, {{Emerging Challenges}}, and {{Future Directions}}},
  shorttitle = {Generative {{Pre-trained Transformer}}},
  author = {Yenduri, Gokul and M, Ramalingam and G, Chemmalar Selvi and Y, Supriya and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and G, Deepti Raj and Jhaveri, Rutvij H. and B, Prabadevi and Wang, Weizheng and Vasilakos, Athanasios V. and Gadekallu, Thippa Reddy},
  date = {2023-05-21},
  eprint = {2305.10435},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10435},
  url = {http://arxiv.org/abs/2305.10435},
  urldate = {2024-06-16},
  abstract = {The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/nschaef/Zotero/storage/NMJXLXDM/Yenduri et al. - 2023 - Generative Pre-trained Transformer A Comprehensiv.pdf;/home/nschaef/Zotero/storage/NILKKBFW/2305.html}
}

@online{ElementsPromptNextra2024,
  title = {Elements of a {{Prompt}} – {{Nextra}}},
  date = {2024-05-31},
  url = {https://www.promptingguide.ai/introduction/elements},
  urldate = {2024-06-17},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/FJEV5R4K/elements.html}
}

@online{PromptEngineeringGuide,
  title = {Prompt {{Engineering Guide}}},
  url = {https://www.promptingguide.ai/techniques},
  urldate = {2024-06-17},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/J9RYF9HI/techniques.html}
}

@online{sahooSystematicSurveyPrompt2024,
  title = {A {{Systematic Survey}} of {{Prompt Engineering}} in {{Large Language Models}}: {{Techniques}} and {{Applications}}},
  shorttitle = {A {{Systematic Survey}} of {{Prompt Engineering}} in {{Large Language Models}}},
  author = {Sahoo, Pranab and Singh, Ayush Kumar and Saha, Sriparna and Jain, Vinija and Mondal, Samrat and Chadha, Aman},
  date = {2024-02-05},
  eprint = {2402.07927},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.07927},
  url = {http://arxiv.org/abs/2402.07927},
  urldate = {2024-06-17},
  abstract = {Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/nschaef/Zotero/storage/CBAT9QC9/Sahoo et al. - 2024 - A Systematic Survey of Prompt Engineering in Large.pdf;/home/nschaef/Zotero/storage/HRLXIAXV/2402.html}
}

@misc{baeldungGuideJUnitBaeldung2016,
  title = {A {{Guide}} to {{JUnit}} 5 {\textbar} {{Baeldung}}},
  author = {{baeldung}},
  year = {2016},
  month = nov,
  urldate = {2024-06-19},
  abstract = {A quick and practical guide to JUnit 5},
  url = {https://www.baeldung.com/junit-5},
  langid = {american},
  file = {/home/nschaef/Zotero/storage/Y5V2DMC2/junit-5.html}
}

@misc{bakharevUnitTestingDefinition2023,
  title = {Unit {{Testing}}: {{Definition}}, {{Examples}}, and {{Critical Best Practices}}},
  shorttitle = {Unit {{Testing}}},
  author = {Bakharev, Nickolay},
  year = {2023},
  month = jul,
  journal = {Bright Security},
  urldate = {2024-06-18},
  abstract = {Learn how unit testing works, see examples of unit tests in popular frameworks, and learn see how to make your unit testing more effective.},
  langid = {american},
  file = {/home/nschaef/Zotero/storage/YZTCW4XW/unit-testing.html}
}

@misc{ikechiHowStructureUnit2021,
  title = {How to {{Structure}} a {{Unit Test}}},
  author = {Ikechi, Chinedu},
  year = {2021},
  month = sep,
  journal = {Medium},
  urldate = {2024-06-18},
  abstract = {Best practices for writing unit tests},
  url = {https://javascript.plainenglish.io/how-to-structure-a-unit-test-9c87f287d1de},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/5J4XA45G/how-to-structure-a-unit-test-9c87f287d1de.html}
}

@misc{JUnit,
  title = {{{JUnit}} 5},
  urldate = {2024-06-19},
  url = {https://junit.org/junit5/},
  file = {/home/nschaef/Zotero/storage/GBP2BZ28/junit5.html}
}

@misc{JUnitUserGuide,
  title = {{{JUnit}} 5 {{User Guide}}},
  urldate = {2024-06-19},
  url = {https://junit.org/junit5/docs/current/user-guide/\#writing-tests-assumptions},
  file = {/home/nschaef/Zotero/storage/L4TCZXZQ/user-guide.html}
}

@misc{SoftwareQualityLab,
  title = {Software {{Quality Lab}}},
  urldate = {2024-06-18},
  url = {https://www.software-quality-lab.com/},
  file = {/home/nschaef/Zotero/storage/Q6YXLXWB/www.software-quality-lab.com.html}
}

@misc{TestingPyramidStrategic2024,
  title = {The Testing Pyramid: {{Strategic}} Software Testing for {{Agile}} Teams},
  shorttitle = {The Testing Pyramid},
  year = {2024},
  month = jan,
  journal = {CircleCI},
  urldate = {2024-06-18},
  abstract = {Enhance your development process with the testing pyramid. Learn tips for adding unit, integration, and E2E tests for improved reliability and quality.},
  url = {https://circleci.com/blog/testing-pyramid/},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/CWSHLAWK/testing-pyramid.html}
}

@inproceedings{tosunEffectivenessUnitTests2018,
  title = {On the {{Effectiveness}} of {{Unit Tests}} in {{Test-driven Development}}},
  booktitle = {International {{Conference}} on {{Software}} and {{System Process}}},
  author = {Tosun, A. and Ahmed, M. and Turhan, B. and Juristo, N.},
  year = {2018},
  urldate = {2024-06-18},
  abstract = {Background: Writing unit tests is one of the primary activities in test-driven development. Yet, the existing reviews report few evidence supporting or refuting the effect of this development approach on test case quality. Lack of ability and skills of developers to produce sufficiently good test cases are also reported as limitations of applying test-driven development in industrial practice. Objective: We investigate the impact of test-driven development on the effectiveness of unit test cases compared to an incremental test last development in an industrial context. Method: We conducted an experiment in an industrial setting with 24 professionals. Professionals followed the two development approaches to implement the tasks. We measure unit test effectiveness in terms of mutation score. We also measure branch and method coverage of test suites to compare our results with the literature. Results: In terms of mutation score, we have found that the test cases written for a test-driven development task have a higher defect detection ability than test cases written for an incremental test-last development task. Subjects wrote test cases that cover more branches on a test-driven development task compared to the other task. However, test cases written for an incremental test-last development task cover more methods than those written for the second task. Conclusion: Our findings are different from previous studies conducted at academic settings. Professionals were able to perform more effective unit testing with test-driven development. Furthermore, we observe that the coverage measure preferred in academic studies reveal different aspects of a development approach. Our results need to be validated in larger industrial contexts.},
  annotation = {Accepted: 2018-04-19T15:59:50Z},
  file = {/home/nschaef/Zotero/storage/EE8EECPE/Tosun et al. - 2018 - On the Effectiveness of Unit Tests in Test-driven .pdf}
}

@article{venkatesanJunitFrameworkUnit,
  title = {Junit Framework for Unit Testing},
  author = {Venkatesan, Praveen Kumar and Rozario, Rikhil Gade and Fiaidhi, Jinan},
  urldate = {2024-06-19},
  abstract = {Testing software before deploying is a mandatory task in SDLC. Various types of testing tools are used to test the software. This research focuses on JUnit framework to perform unit testing for Java applications. We have developed a Banking Inventory},
  file = {/home/nschaef/Zotero/storage/SX62KIPZ/Venkatesan et al. - Junit framework for unit testing.pdf}
}

@misc{MetricDefinition,
  title = {Metric Definition},
  urldate = {2024-06-21},
  abstract = {SonarQube analysis produces many types of code analysis metrics.},
  url = {https://docs.sonarsource.com/sonarqube/latest/user-guide/metric-definitions/},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/UDQHUDJE/metric-definitions.html}
}

@misc{SonarQube10,
  title = {{{SonarQube}} 10.5},
  urldate = {2024-06-21},
  abstract = {SonarQube is a self-managed, automatic code review tool that systematically helps you deliver clean code.},
  url = {https://docs.sonarsource.com/sonarqube/latest/},
  langid = {english},
  file = {/home/nschaef/Zotero/storage/LDVKDSVK/latest.html}
}

@misc{WhatReasonableCode,
  title = {What Is a Reasonable Code Coverage \% for Unit Tests (and Why)?},
  journal = {CodiumAI},
  urldate = {2024-06-21},
  abstract = {Need to know What is a reasonable code coverage \% for unit tests (and why)? Check our answer on CodiumAI General Q\&A's},
  url = {https://www.codium.ai/question/what-is-a-reasonable-code-coverage-for-unit-tests-and-why/},
  langid = {american},
  file = {/home/nschaef/Zotero/storage/R3N7DVG4/what-is-a-reasonable-code-coverage-for-unit-tests-and-why.html}
}
